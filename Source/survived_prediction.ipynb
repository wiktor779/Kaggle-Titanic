{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>...</th>\n",
       "      <th>SOTON/O.Q.</th>\n",
       "      <th>2.</th>\n",
       "      <th>STON/O</th>\n",
       "      <th>W./C.</th>\n",
       "      <th>CA.</th>\n",
       "      <th>A/5</th>\n",
       "      <th>SC/PARIS</th>\n",
       "      <th>2343</th>\n",
       "      <th>CA</th>\n",
       "      <th>A/5.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.270689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.270696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.349900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.349900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.009324</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass       Age  SibSp     Parch      Fare  female  \\\n",
       "PassengerId                                                                  \n",
       "1                 0.0     1.0  0.271174    0.2  0.000000  0.014151     0.0   \n",
       "2                 1.0     0.0  0.472229    0.2  0.000000  0.139136     1.0   \n",
       "3                 1.0     1.0  0.321438    0.0  0.000000  0.015469     1.0   \n",
       "4                 1.0     0.0  0.434531    0.2  0.000000  0.103644     1.0   \n",
       "5                 0.0     1.0  0.434531    0.0  0.000000  0.015713     0.0   \n",
       "...               ...     ...       ...    ...       ...       ...     ...   \n",
       "1300              NaN     1.0  0.270689    0.0  0.000000  0.015070     1.0   \n",
       "1302              NaN     1.0  0.270696    0.0  0.000000  0.015127     1.0   \n",
       "1305              NaN     1.0  0.349900    0.0  0.000000  0.015713     0.0   \n",
       "1308              NaN     1.0  0.349900    0.0  0.000000  0.015713     0.0   \n",
       "1309              NaN     1.0 -0.009324    0.2  0.166667  0.043640     0.0   \n",
       "\n",
       "             male    C    Q  ...  SOTON/O.Q.   2.  STON/O  W./C.  CA.  A/5  \\\n",
       "PassengerId                  ...                                             \n",
       "1             1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  1.0   \n",
       "2             0.0  1.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "3             0.0  0.0  0.0  ...         0.0  1.0     1.0    0.0  0.0  0.0   \n",
       "4             0.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "5             1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "...           ...  ...  ...  ...         ...  ...     ...    ...  ...  ...   \n",
       "1300          0.0  0.0  1.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "1302          0.0  0.0  1.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "1305          1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "1308          1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "1309          1.0  1.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "\n",
       "             SC/PARIS  2343   CA  A/5.  \n",
       "PassengerId                             \n",
       "1                 0.0   0.0  0.0   0.0  \n",
       "2                 0.0   0.0  0.0   0.0  \n",
       "3                 0.0   0.0  0.0   0.0  \n",
       "4                 0.0   0.0  0.0   0.0  \n",
       "5                 0.0   0.0  0.0   0.0  \n",
       "...               ...   ...  ...   ...  \n",
       "1300              0.0   0.0  0.0   0.0  \n",
       "1302              0.0   0.0  0.0   0.0  \n",
       "1305              0.0   0.0  0.0   0.0  \n",
       "1308              0.0   0.0  0.0   0.0  \n",
       "1309              0.0   0.0  0.0   0.0  \n",
       "\n",
       "[1309 rows x 26 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from keras.optimizers import RMSprop\n",
    "seed(1)\n",
    "\n",
    "normalized_data = pd.read_csv(\"../Output/normalized_data_with_predicted_age.csv\", index_col=0)\n",
    "normalized_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>...</th>\n",
       "      <th>SOTON/O.Q.</th>\n",
       "      <th>2.</th>\n",
       "      <th>STON/O</th>\n",
       "      <th>W./C.</th>\n",
       "      <th>CA.</th>\n",
       "      <th>A/5</th>\n",
       "      <th>SC/PARIS</th>\n",
       "      <th>2343</th>\n",
       "      <th>CA</th>\n",
       "      <th>A/5.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.322933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.191448</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.135753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.349832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.349907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.281485</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.045771</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass       Age  SibSp     Parch      Fare  female  \\\n",
       "PassengerId                                                                  \n",
       "1                 0.0     1.0  0.271174    0.2  0.000000  0.014151     0.0   \n",
       "2                 1.0     0.0  0.472229    0.2  0.000000  0.139136     1.0   \n",
       "3                 1.0     1.0  0.321438    0.0  0.000000  0.015469     1.0   \n",
       "4                 1.0     0.0  0.434531    0.2  0.000000  0.103644     1.0   \n",
       "5                 0.0     1.0  0.434531    0.0  0.000000  0.015713     0.0   \n",
       "...               ...     ...       ...    ...       ...       ...     ...   \n",
       "860               0.0     1.0  0.322933    0.0  0.000000  0.014110     0.0   \n",
       "864               0.0     1.0  0.191448    1.6  0.333333  0.135753     1.0   \n",
       "869               0.0     1.0  0.349832    0.0  0.000000  0.018543     0.0   \n",
       "879               0.0     1.0  0.349907    0.0  0.000000  0.015412     0.0   \n",
       "889               0.0     1.0  0.281485    0.2  0.333333  0.045771     1.0   \n",
       "\n",
       "             male    C    Q  ...  SOTON/O.Q.   2.  STON/O  W./C.  CA.  A/5  \\\n",
       "PassengerId                  ...                                             \n",
       "1             1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  1.0   \n",
       "2             0.0  1.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "3             0.0  0.0  0.0  ...         0.0  1.0     1.0    0.0  0.0  0.0   \n",
       "4             0.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "5             1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "...           ...  ...  ...  ...         ...  ...     ...    ...  ...  ...   \n",
       "860           1.0  1.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "864           0.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  1.0  0.0   \n",
       "869           1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "879           1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "889           0.0  0.0  0.0  ...         0.0  0.0     0.0    1.0  0.0  0.0   \n",
       "\n",
       "             SC/PARIS  2343   CA  A/5.  \n",
       "PassengerId                             \n",
       "1                 0.0   0.0  0.0   0.0  \n",
       "2                 0.0   0.0  0.0   0.0  \n",
       "3                 0.0   0.0  0.0   0.0  \n",
       "4                 0.0   0.0  0.0   0.0  \n",
       "5                 0.0   0.0  0.0   0.0  \n",
       "...               ...   ...  ...   ...  \n",
       "860               0.0   0.0  0.0   0.0  \n",
       "864               0.0   1.0  1.0   0.0  \n",
       "869               0.0   0.0  0.0   0.0  \n",
       "879               0.0   0.0  0.0   0.0  \n",
       "889               0.0   0.0  0.0   0.0  \n",
       "\n",
       "[891 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train_data = normalized_data[normalized_data.Survived.notna()]\n",
    "normalized_test_data = normalized_data[normalized_data.Survived.isna()]\n",
    "normalized_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 580us/step - loss: 0.6560 - acc: 0.6309 - val_loss: 0.5543 - val_acc: 0.8031\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 186us/step - loss: 0.5642 - acc: 0.7592 - val_loss: 0.4841 - val_acc: 0.8110\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.5238 - acc: 0.7788 - val_loss: 0.4637 - val_acc: 0.8031\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.5096 - acc: 0.7814 - val_loss: 0.4498 - val_acc: 0.8031\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 151us/step - loss: 0.5166 - acc: 0.7801 - val_loss: 0.4440 - val_acc: 0.8110\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4892 - acc: 0.7932 - val_loss: 0.4520 - val_acc: 0.8031\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 150us/step - loss: 0.4741 - acc: 0.8010 - val_loss: 0.4477 - val_acc: 0.7953\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 162us/step - loss: 0.4826 - acc: 0.7919 - val_loss: 0.4364 - val_acc: 0.8268\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.4689 - acc: 0.7958 - val_loss: 0.4247 - val_acc: 0.8346\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4711 - acc: 0.7919 - val_loss: 0.4262 - val_acc: 0.8346\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 146us/step - loss: 0.4611 - acc: 0.8050 - val_loss: 0.4345 - val_acc: 0.8189\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4538 - acc: 0.8050 - val_loss: 0.4375 - val_acc: 0.8189\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4583 - acc: 0.8063 - val_loss: 0.4332 - val_acc: 0.8031\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 158us/step - loss: 0.4712 - acc: 0.8076 - val_loss: 0.4312 - val_acc: 0.8031\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4607 - acc: 0.8154 - val_loss: 0.4392 - val_acc: 0.8189\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 152us/step - loss: 0.4594 - acc: 0.8089 - val_loss: 0.4313 - val_acc: 0.8031\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 169us/step - loss: 0.4493 - acc: 0.8141 - val_loss: 0.4349 - val_acc: 0.8031\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4671 - acc: 0.8050 - val_loss: 0.4368 - val_acc: 0.8110\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4452 - acc: 0.8220 - val_loss: 0.4399 - val_acc: 0.8189\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.4530 - acc: 0.8154 - val_loss: 0.4358 - val_acc: 0.8031\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 192us/step - loss: 0.4386 - acc: 0.8168 - val_loss: 0.4346 - val_acc: 0.7953\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 206us/step - loss: 0.4418 - acc: 0.8220 - val_loss: 0.4315 - val_acc: 0.8110\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 183us/step - loss: 0.4409 - acc: 0.8076 - val_loss: 0.4364 - val_acc: 0.8268\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 187us/step - loss: 0.4281 - acc: 0.8141 - val_loss: 0.4344 - val_acc: 0.8268\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 151us/step - loss: 0.4482 - acc: 0.8128 - val_loss: 0.4282 - val_acc: 0.8189\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 170us/step - loss: 0.4299 - acc: 0.8168 - val_loss: 0.4343 - val_acc: 0.8110\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4306 - acc: 0.8259 - val_loss: 0.4288 - val_acc: 0.8189\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 165us/step - loss: 0.4246 - acc: 0.8168 - val_loss: 0.4281 - val_acc: 0.8110\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 176us/step - loss: 0.4195 - acc: 0.8259 - val_loss: 0.4281 - val_acc: 0.8189\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 201us/step - loss: 0.4285 - acc: 0.8194 - val_loss: 0.4421 - val_acc: 0.8346\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 211us/step - loss: 0.4301 - acc: 0.8220 - val_loss: 0.4339 - val_acc: 0.8189\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 158us/step - loss: 0.4346 - acc: 0.8233 - val_loss: 0.4276 - val_acc: 0.8110\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4356 - acc: 0.8207 - val_loss: 0.4239 - val_acc: 0.8189\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4287 - acc: 0.8325 - val_loss: 0.4281 - val_acc: 0.8425\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4231 - acc: 0.8207 - val_loss: 0.4257 - val_acc: 0.8189\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4183 - acc: 0.8377 - val_loss: 0.4226 - val_acc: 0.8189\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 170us/step - loss: 0.4260 - acc: 0.8259 - val_loss: 0.4272 - val_acc: 0.8268\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4180 - acc: 0.8455 - val_loss: 0.4200 - val_acc: 0.8346\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4090 - acc: 0.8416 - val_loss: 0.4257 - val_acc: 0.8425\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4103 - acc: 0.8377 - val_loss: 0.4252 - val_acc: 0.8346\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 165us/step - loss: 0.4250 - acc: 0.8246 - val_loss: 0.4215 - val_acc: 0.8268\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 172us/step - loss: 0.4128 - acc: 0.8416 - val_loss: 0.4336 - val_acc: 0.8346\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 171us/step - loss: 0.4216 - acc: 0.8246 - val_loss: 0.4230 - val_acc: 0.8425\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 180us/step - loss: 0.4056 - acc: 0.8285 - val_loss: 0.4158 - val_acc: 0.8268\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4033 - acc: 0.8495 - val_loss: 0.4228 - val_acc: 0.8346\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4115 - acc: 0.8429 - val_loss: 0.4211 - val_acc: 0.8031\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4028 - acc: 0.8390 - val_loss: 0.4223 - val_acc: 0.8346\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4066 - acc: 0.8429 - val_loss: 0.4249 - val_acc: 0.8110\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4354 - acc: 0.8246 - val_loss: 0.4295 - val_acc: 0.8346\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4236 - acc: 0.8285 - val_loss: 0.4191 - val_acc: 0.8425\n",
      "processing fold # 1\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 1s 809us/step - loss: 0.6442 - acc: 0.6296 - val_loss: 0.5884 - val_acc: 0.7087\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.5303 - acc: 0.7631 - val_loss: 0.5302 - val_acc: 0.7717\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.5039 - acc: 0.7853 - val_loss: 0.5190 - val_acc: 0.7638\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 191us/step - loss: 0.4908 - acc: 0.7932 - val_loss: 0.5141 - val_acc: 0.7638\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 196us/step - loss: 0.4849 - acc: 0.7984 - val_loss: 0.5145 - val_acc: 0.7480\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 178us/step - loss: 0.4629 - acc: 0.8010 - val_loss: 0.5161 - val_acc: 0.7480\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 188us/step - loss: 0.4540 - acc: 0.8168 - val_loss: 0.5134 - val_acc: 0.7717\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 165us/step - loss: 0.4686 - acc: 0.8050 - val_loss: 0.5137 - val_acc: 0.7480\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 198us/step - loss: 0.4658 - acc: 0.7997 - val_loss: 0.5133 - val_acc: 0.7480\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 196us/step - loss: 0.4438 - acc: 0.8141 - val_loss: 0.5189 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 184us/step - loss: 0.4493 - acc: 0.8063 - val_loss: 0.5109 - val_acc: 0.7717\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 178us/step - loss: 0.4513 - acc: 0.8115 - val_loss: 0.5093 - val_acc: 0.7559\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 182us/step - loss: 0.4422 - acc: 0.8128 - val_loss: 0.5113 - val_acc: 0.7795\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 157us/step - loss: 0.4362 - acc: 0.8207 - val_loss: 0.5148 - val_acc: 0.7795\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4485 - acc: 0.8154 - val_loss: 0.5085 - val_acc: 0.7795\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4274 - acc: 0.8259 - val_loss: 0.5108 - val_acc: 0.7717\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 177us/step - loss: 0.4164 - acc: 0.8220 - val_loss: 0.5174 - val_acc: 0.7795\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 266us/step - loss: 0.4109 - acc: 0.8325 - val_loss: 0.5287 - val_acc: 0.7795\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 246us/step - loss: 0.4323 - acc: 0.8194 - val_loss: 0.5251 - val_acc: 0.7795\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 239us/step - loss: 0.4407 - acc: 0.8220 - val_loss: 0.5220 - val_acc: 0.7795\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 165us/step - loss: 0.4097 - acc: 0.8285 - val_loss: 0.5326 - val_acc: 0.7795\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 206us/step - loss: 0.4169 - acc: 0.8416 - val_loss: 0.5368 - val_acc: 0.7795\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 180us/step - loss: 0.4158 - acc: 0.8233 - val_loss: 0.5248 - val_acc: 0.7795\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 159us/step - loss: 0.4312 - acc: 0.8272 - val_loss: 0.5287 - val_acc: 0.7795\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 146us/step - loss: 0.4022 - acc: 0.8233 - val_loss: 0.5353 - val_acc: 0.7795\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4090 - acc: 0.8272 - val_loss: 0.5363 - val_acc: 0.7874\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4025 - acc: 0.8325 - val_loss: 0.5432 - val_acc: 0.7795\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 197us/step - loss: 0.4196 - acc: 0.8259 - val_loss: 0.5346 - val_acc: 0.7953\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 244us/step - loss: 0.4130 - acc: 0.8325 - val_loss: 0.5482 - val_acc: 0.7874\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 191us/step - loss: 0.4042 - acc: 0.8312 - val_loss: 0.5464 - val_acc: 0.7953\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 244us/step - loss: 0.4084 - acc: 0.8364 - val_loss: 0.5480 - val_acc: 0.7953\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 266us/step - loss: 0.4180 - acc: 0.8233 - val_loss: 0.5491 - val_acc: 0.7874\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 264us/step - loss: 0.4056 - acc: 0.8285 - val_loss: 0.5623 - val_acc: 0.7874\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 219us/step - loss: 0.4007 - acc: 0.8390 - val_loss: 0.5565 - val_acc: 0.7953\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 196us/step - loss: 0.3914 - acc: 0.8390 - val_loss: 0.5659 - val_acc: 0.7874\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 196us/step - loss: 0.4016 - acc: 0.8325 - val_loss: 0.5737 - val_acc: 0.7953\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 198us/step - loss: 0.3831 - acc: 0.8403 - val_loss: 0.5830 - val_acc: 0.7953\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 247us/step - loss: 0.3881 - acc: 0.8416 - val_loss: 0.5830 - val_acc: 0.7953\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 264us/step - loss: 0.3920 - acc: 0.8508 - val_loss: 0.5777 - val_acc: 0.8031\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 238us/step - loss: 0.3855 - acc: 0.8482 - val_loss: 0.5895 - val_acc: 0.8031\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 248us/step - loss: 0.3844 - acc: 0.8390 - val_loss: 0.5968 - val_acc: 0.8110\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 210us/step - loss: 0.3957 - acc: 0.8364 - val_loss: 0.5886 - val_acc: 0.7953\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 218us/step - loss: 0.3952 - acc: 0.8495 - val_loss: 0.5940 - val_acc: 0.7953\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 249us/step - loss: 0.3834 - acc: 0.8403 - val_loss: 0.6103 - val_acc: 0.7953\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 214us/step - loss: 0.3894 - acc: 0.8455 - val_loss: 0.6063 - val_acc: 0.8031\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 201us/step - loss: 0.3961 - acc: 0.8364 - val_loss: 0.5958 - val_acc: 0.8031\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 294us/step - loss: 0.3858 - acc: 0.8469 - val_loss: 0.5965 - val_acc: 0.7953\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 283us/step - loss: 0.3900 - acc: 0.8325 - val_loss: 0.6164 - val_acc: 0.7953\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 294us/step - loss: 0.3759 - acc: 0.8521 - val_loss: 0.6259 - val_acc: 0.8031\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 212us/step - loss: 0.3987 - acc: 0.8351 - val_loss: 0.6074 - val_acc: 0.8031\n",
      "processing fold # 2\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 513us/step - loss: 0.6453 - acc: 0.6309 - val_loss: 0.5935 - val_acc: 0.7795\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.5481 - acc: 0.7552 - val_loss: 0.5062 - val_acc: 0.7953\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.5314 - acc: 0.7775 - val_loss: 0.4756 - val_acc: 0.7874\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 160us/step - loss: 0.4915 - acc: 0.7827 - val_loss: 0.4636 - val_acc: 0.7953\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4882 - acc: 0.7880 - val_loss: 0.4553 - val_acc: 0.7953\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4826 - acc: 0.7814 - val_loss: 0.4473 - val_acc: 0.8031\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4763 - acc: 0.7971 - val_loss: 0.4452 - val_acc: 0.8031\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.4681 - acc: 0.7827 - val_loss: 0.4424 - val_acc: 0.8031\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4669 - acc: 0.8089 - val_loss: 0.4356 - val_acc: 0.8031\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4712 - acc: 0.8010 - val_loss: 0.4357 - val_acc: 0.8031\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4551 - acc: 0.7997 - val_loss: 0.4330 - val_acc: 0.8031\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4400 - acc: 0.8037 - val_loss: 0.4237 - val_acc: 0.8110\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 209us/step - loss: 0.4490 - acc: 0.7971 - val_loss: 0.4232 - val_acc: 0.8110\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4446 - acc: 0.8089 - val_loss: 0.4274 - val_acc: 0.8031\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4532 - acc: 0.8102 - val_loss: 0.4157 - val_acc: 0.8110\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4490 - acc: 0.8037 - val_loss: 0.4124 - val_acc: 0.8031\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 159us/step - loss: 0.4522 - acc: 0.8037 - val_loss: 0.4112 - val_acc: 0.8110\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 265us/step - loss: 0.4481 - acc: 0.8050 - val_loss: 0.4093 - val_acc: 0.8189\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 178us/step - loss: 0.4501 - acc: 0.8037 - val_loss: 0.4060 - val_acc: 0.8189\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 155us/step - loss: 0.4342 - acc: 0.8259 - val_loss: 0.4017 - val_acc: 0.8346\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 209us/step - loss: 0.4368 - acc: 0.8181 - val_loss: 0.3987 - val_acc: 0.8268\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 220us/step - loss: 0.4438 - acc: 0.8207 - val_loss: 0.3991 - val_acc: 0.8268\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 258us/step - loss: 0.4384 - acc: 0.8181 - val_loss: 0.3978 - val_acc: 0.8268\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 172us/step - loss: 0.4494 - acc: 0.8207 - val_loss: 0.3993 - val_acc: 0.8268\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4384 - acc: 0.8233 - val_loss: 0.3954 - val_acc: 0.8346\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4289 - acc: 0.8233 - val_loss: 0.3993 - val_acc: 0.8346\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4295 - acc: 0.8338 - val_loss: 0.3957 - val_acc: 0.8268\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4327 - acc: 0.8285 - val_loss: 0.3889 - val_acc: 0.8268\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4308 - acc: 0.8246 - val_loss: 0.3908 - val_acc: 0.8268\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4244 - acc: 0.8194 - val_loss: 0.3889 - val_acc: 0.8268\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4111 - acc: 0.8285 - val_loss: 0.3895 - val_acc: 0.8346\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4208 - acc: 0.8338 - val_loss: 0.3913 - val_acc: 0.8425\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4061 - acc: 0.8338 - val_loss: 0.3867 - val_acc: 0.8346\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4295 - acc: 0.8298 - val_loss: 0.3875 - val_acc: 0.8504\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4201 - acc: 0.8312 - val_loss: 0.3888 - val_acc: 0.8346\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 159us/step - loss: 0.4289 - acc: 0.8298 - val_loss: 0.3889 - val_acc: 0.8425\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4214 - acc: 0.8338 - val_loss: 0.3858 - val_acc: 0.8346\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 149us/step - loss: 0.4049 - acc: 0.8377 - val_loss: 0.3825 - val_acc: 0.8346\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 150us/step - loss: 0.4258 - acc: 0.8194 - val_loss: 0.3879 - val_acc: 0.8346\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4143 - acc: 0.8364 - val_loss: 0.3829 - val_acc: 0.8504\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4214 - acc: 0.8377 - val_loss: 0.3858 - val_acc: 0.8425\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 202us/step - loss: 0.4234 - acc: 0.8364 - val_loss: 0.3910 - val_acc: 0.8346\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4298 - acc: 0.8364 - val_loss: 0.3903 - val_acc: 0.8346\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 167us/step - loss: 0.4206 - acc: 0.8377 - val_loss: 0.3871 - val_acc: 0.8346\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 155us/step - loss: 0.4018 - acc: 0.8403 - val_loss: 0.3827 - val_acc: 0.8425\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 185us/step - loss: 0.4271 - acc: 0.8377 - val_loss: 0.3795 - val_acc: 0.8425\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4208 - acc: 0.8377 - val_loss: 0.3811 - val_acc: 0.8425\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 154us/step - loss: 0.4135 - acc: 0.8364 - val_loss: 0.3799 - val_acc: 0.8346\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 173us/step - loss: 0.4184 - acc: 0.8416 - val_loss: 0.3805 - val_acc: 0.8268\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 152us/step - loss: 0.4167 - acc: 0.8416 - val_loss: 0.3821 - val_acc: 0.8425\n",
      "processing fold # 3\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 605us/step - loss: 0.6107 - acc: 0.6479 - val_loss: 0.5829 - val_acc: 0.7480\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.5479 - acc: 0.7369 - val_loss: 0.5536 - val_acc: 0.7480\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.5206 - acc: 0.7775 - val_loss: 0.5524 - val_acc: 0.7480\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.5177 - acc: 0.7840 - val_loss: 0.5532 - val_acc: 0.7480\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4975 - acc: 0.7906 - val_loss: 0.5366 - val_acc: 0.7559\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4678 - acc: 0.7945 - val_loss: 0.5362 - val_acc: 0.7559\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4815 - acc: 0.8010 - val_loss: 0.5204 - val_acc: 0.7559\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4618 - acc: 0.8115 - val_loss: 0.5128 - val_acc: 0.7559\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4925 - acc: 0.7984 - val_loss: 0.4971 - val_acc: 0.7638\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4623 - acc: 0.8154 - val_loss: 0.4916 - val_acc: 0.7638\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 182us/step - loss: 0.4580 - acc: 0.8141 - val_loss: 0.4972 - val_acc: 0.7717\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 170us/step - loss: 0.4563 - acc: 0.8154 - val_loss: 0.4812 - val_acc: 0.7795\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4560 - acc: 0.8076 - val_loss: 0.4746 - val_acc: 0.7874\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4412 - acc: 0.8128 - val_loss: 0.4772 - val_acc: 0.7795\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4658 - acc: 0.8089 - val_loss: 0.4705 - val_acc: 0.7874\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4324 - acc: 0.8115 - val_loss: 0.4754 - val_acc: 0.7953\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 167us/step - loss: 0.4382 - acc: 0.8089 - val_loss: 0.4669 - val_acc: 0.7874\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4452 - acc: 0.8102 - val_loss: 0.4677 - val_acc: 0.7874\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4513 - acc: 0.8141 - val_loss: 0.4722 - val_acc: 0.7874\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4363 - acc: 0.8194 - val_loss: 0.4679 - val_acc: 0.7953\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4442 - acc: 0.8168 - val_loss: 0.4614 - val_acc: 0.7874\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4290 - acc: 0.8050 - val_loss: 0.4633 - val_acc: 0.7795\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 156us/step - loss: 0.4367 - acc: 0.8115 - val_loss: 0.4588 - val_acc: 0.7874\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4248 - acc: 0.8220 - val_loss: 0.4548 - val_acc: 0.7795\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4355 - acc: 0.8141 - val_loss: 0.4587 - val_acc: 0.7795\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4131 - acc: 0.8338 - val_loss: 0.4718 - val_acc: 0.7795\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4355 - acc: 0.8181 - val_loss: 0.4645 - val_acc: 0.7874\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4311 - acc: 0.8207 - val_loss: 0.4561 - val_acc: 0.7874\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4385 - acc: 0.8207 - val_loss: 0.4631 - val_acc: 0.7874\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.4442 - acc: 0.8181 - val_loss: 0.4613 - val_acc: 0.7795\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 138us/step - loss: 0.4251 - acc: 0.8285 - val_loss: 0.4569 - val_acc: 0.7874\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4286 - acc: 0.8194 - val_loss: 0.4623 - val_acc: 0.7874\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4189 - acc: 0.8298 - val_loss: 0.4621 - val_acc: 0.8110\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4241 - acc: 0.8194 - val_loss: 0.4661 - val_acc: 0.7874\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4306 - acc: 0.8233 - val_loss: 0.4622 - val_acc: 0.7953\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4305 - acc: 0.8233 - val_loss: 0.4568 - val_acc: 0.7953\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4303 - acc: 0.8272 - val_loss: 0.4596 - val_acc: 0.7874\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.4300 - acc: 0.8312 - val_loss: 0.4650 - val_acc: 0.7874\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4193 - acc: 0.8429 - val_loss: 0.4687 - val_acc: 0.7953\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 146us/step - loss: 0.4268 - acc: 0.8325 - val_loss: 0.4700 - val_acc: 0.7874\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4213 - acc: 0.8272 - val_loss: 0.4615 - val_acc: 0.7874\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4207 - acc: 0.8325 - val_loss: 0.4699 - val_acc: 0.7874\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4181 - acc: 0.8285 - val_loss: 0.4633 - val_acc: 0.7874\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4228 - acc: 0.8220 - val_loss: 0.4690 - val_acc: 0.7953\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4231 - acc: 0.8259 - val_loss: 0.4674 - val_acc: 0.7953\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4218 - acc: 0.8298 - val_loss: 0.4681 - val_acc: 0.7874\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.4274 - acc: 0.8298 - val_loss: 0.4625 - val_acc: 0.7953\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4130 - acc: 0.8312 - val_loss: 0.4657 - val_acc: 0.7874\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4164 - acc: 0.8312 - val_loss: 0.4606 - val_acc: 0.7874\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4135 - acc: 0.8325 - val_loss: 0.4597 - val_acc: 0.7874\n",
      "processing fold # 4\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 591us/step - loss: 0.6254 - acc: 0.6846 - val_loss: 0.5444 - val_acc: 0.7717\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.5600 - acc: 0.7317 - val_loss: 0.4851 - val_acc: 0.7795\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.5343 - acc: 0.7736 - val_loss: 0.4622 - val_acc: 0.7795\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.5117 - acc: 0.7853 - val_loss: 0.4556 - val_acc: 0.7795\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.5153 - acc: 0.7840 - val_loss: 0.4486 - val_acc: 0.7953\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.4988 - acc: 0.7893 - val_loss: 0.4435 - val_acc: 0.8031\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4962 - acc: 0.7801 - val_loss: 0.4395 - val_acc: 0.8031\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.4912 - acc: 0.7945 - val_loss: 0.4423 - val_acc: 0.7953\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4849 - acc: 0.7866 - val_loss: 0.4394 - val_acc: 0.7953\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4595 - acc: 0.8207 - val_loss: 0.4330 - val_acc: 0.7953\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4577 - acc: 0.8037 - val_loss: 0.4323 - val_acc: 0.8031\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4630 - acc: 0.8037 - val_loss: 0.4271 - val_acc: 0.8110\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4628 - acc: 0.8154 - val_loss: 0.4303 - val_acc: 0.8110\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4525 - acc: 0.8102 - val_loss: 0.4196 - val_acc: 0.8268\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4610 - acc: 0.8233 - val_loss: 0.4189 - val_acc: 0.8110\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4593 - acc: 0.8168 - val_loss: 0.4200 - val_acc: 0.8110\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4521 - acc: 0.8207 - val_loss: 0.4168 - val_acc: 0.8031\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 156us/step - loss: 0.4505 - acc: 0.8207 - val_loss: 0.4185 - val_acc: 0.7953\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4427 - acc: 0.8207 - val_loss: 0.4182 - val_acc: 0.8110\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4397 - acc: 0.8181 - val_loss: 0.4114 - val_acc: 0.8189\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4381 - acc: 0.8233 - val_loss: 0.4127 - val_acc: 0.8110\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4553 - acc: 0.8168 - val_loss: 0.4155 - val_acc: 0.8031\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4351 - acc: 0.8403 - val_loss: 0.4161 - val_acc: 0.8031\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4378 - acc: 0.8207 - val_loss: 0.4200 - val_acc: 0.8031\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4351 - acc: 0.8338 - val_loss: 0.4201 - val_acc: 0.8031\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4110 - acc: 0.8312 - val_loss: 0.4238 - val_acc: 0.8031\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 155us/step - loss: 0.4321 - acc: 0.8377 - val_loss: 0.4257 - val_acc: 0.8031\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 193us/step - loss: 0.4191 - acc: 0.8338 - val_loss: 0.4271 - val_acc: 0.8031\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4475 - acc: 0.8233 - val_loss: 0.4304 - val_acc: 0.8031\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4246 - acc: 0.8325 - val_loss: 0.4283 - val_acc: 0.7953\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4249 - acc: 0.8272 - val_loss: 0.4254 - val_acc: 0.8031\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 210us/step - loss: 0.4266 - acc: 0.8325 - val_loss: 0.4241 - val_acc: 0.8110\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 258us/step - loss: 0.4273 - acc: 0.8312 - val_loss: 0.4291 - val_acc: 0.7953\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 276us/step - loss: 0.4380 - acc: 0.8272 - val_loss: 0.4304 - val_acc: 0.8031\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4165 - acc: 0.8325 - val_loss: 0.4334 - val_acc: 0.8110\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4197 - acc: 0.8298 - val_loss: 0.4330 - val_acc: 0.8031\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4158 - acc: 0.8312 - val_loss: 0.4380 - val_acc: 0.8031\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4312 - acc: 0.8338 - val_loss: 0.4395 - val_acc: 0.7953\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4302 - acc: 0.8390 - val_loss: 0.4353 - val_acc: 0.8031\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4135 - acc: 0.8312 - val_loss: 0.4409 - val_acc: 0.8110\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 193us/step - loss: 0.4130 - acc: 0.8403 - val_loss: 0.4346 - val_acc: 0.8031\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4243 - acc: 0.8312 - val_loss: 0.4359 - val_acc: 0.8110\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4259 - acc: 0.8338 - val_loss: 0.4361 - val_acc: 0.8110\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4099 - acc: 0.8442 - val_loss: 0.4387 - val_acc: 0.8031\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4125 - acc: 0.8403 - val_loss: 0.4372 - val_acc: 0.8110\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4122 - acc: 0.8312 - val_loss: 0.4395 - val_acc: 0.8110\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4162 - acc: 0.8364 - val_loss: 0.4440 - val_acc: 0.8031\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4150 - acc: 0.8429 - val_loss: 0.4481 - val_acc: 0.8031\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4113 - acc: 0.8390 - val_loss: 0.4460 - val_acc: 0.8031\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4177 - acc: 0.8325 - val_loss: 0.4510 - val_acc: 0.8031\n",
      "processing fold # 5\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 485us/step - loss: 0.6197 - acc: 0.6571 - val_loss: 0.5548 - val_acc: 0.7480\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.5660 - acc: 0.7356 - val_loss: 0.5088 - val_acc: 0.7874\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.5331 - acc: 0.7723 - val_loss: 0.4900 - val_acc: 0.7795\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.5086 - acc: 0.7736 - val_loss: 0.4786 - val_acc: 0.7795\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4924 - acc: 0.7853 - val_loss: 0.4670 - val_acc: 0.7874\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4983 - acc: 0.7893 - val_loss: 0.4600 - val_acc: 0.7953\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4808 - acc: 0.7827 - val_loss: 0.4537 - val_acc: 0.7953\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.4753 - acc: 0.7971 - val_loss: 0.4471 - val_acc: 0.8110\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4768 - acc: 0.7906 - val_loss: 0.4443 - val_acc: 0.8189\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4863 - acc: 0.7866 - val_loss: 0.4378 - val_acc: 0.8189\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4593 - acc: 0.7984 - val_loss: 0.4314 - val_acc: 0.8189\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4729 - acc: 0.7880 - val_loss: 0.4324 - val_acc: 0.8189\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4568 - acc: 0.8063 - val_loss: 0.4245 - val_acc: 0.8268\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4549 - acc: 0.8063 - val_loss: 0.4210 - val_acc: 0.8268\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4584 - acc: 0.8102 - val_loss: 0.4231 - val_acc: 0.8189\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4506 - acc: 0.8050 - val_loss: 0.4154 - val_acc: 0.8189\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4454 - acc: 0.8194 - val_loss: 0.4119 - val_acc: 0.8189\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4290 - acc: 0.8220 - val_loss: 0.4098 - val_acc: 0.8189\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4371 - acc: 0.8181 - val_loss: 0.4075 - val_acc: 0.8268\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.4400 - acc: 0.8194 - val_loss: 0.4134 - val_acc: 0.8189\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4433 - acc: 0.8168 - val_loss: 0.4048 - val_acc: 0.8425\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4449 - acc: 0.8154 - val_loss: 0.4109 - val_acc: 0.8346\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4119 - acc: 0.8154 - val_loss: 0.4003 - val_acc: 0.8583\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4338 - acc: 0.8272 - val_loss: 0.3991 - val_acc: 0.8425\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4421 - acc: 0.8207 - val_loss: 0.4029 - val_acc: 0.8346\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4313 - acc: 0.8246 - val_loss: 0.3953 - val_acc: 0.8583\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4364 - acc: 0.8181 - val_loss: 0.3978 - val_acc: 0.8425\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4374 - acc: 0.8272 - val_loss: 0.3965 - val_acc: 0.8504\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4346 - acc: 0.8285 - val_loss: 0.4072 - val_acc: 0.8346\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4414 - acc: 0.8207 - val_loss: 0.3996 - val_acc: 0.8425\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4297 - acc: 0.8272 - val_loss: 0.3947 - val_acc: 0.8504\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4389 - acc: 0.8141 - val_loss: 0.3964 - val_acc: 0.8504\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4125 - acc: 0.8246 - val_loss: 0.3988 - val_acc: 0.8504\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4293 - acc: 0.8338 - val_loss: 0.3962 - val_acc: 0.8425\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4275 - acc: 0.8259 - val_loss: 0.3934 - val_acc: 0.8583\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4245 - acc: 0.8298 - val_loss: 0.3971 - val_acc: 0.8425\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4135 - acc: 0.8377 - val_loss: 0.3940 - val_acc: 0.8583\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4391 - acc: 0.8298 - val_loss: 0.3973 - val_acc: 0.8425\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4177 - acc: 0.8325 - val_loss: 0.3927 - val_acc: 0.8583\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4014 - acc: 0.8325 - val_loss: 0.3925 - val_acc: 0.8504\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4288 - acc: 0.8181 - val_loss: 0.3897 - val_acc: 0.8504\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4316 - acc: 0.8207 - val_loss: 0.3926 - val_acc: 0.8504\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4177 - acc: 0.8338 - val_loss: 0.3869 - val_acc: 0.8583\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4133 - acc: 0.8416 - val_loss: 0.3868 - val_acc: 0.8504\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 213us/step - loss: 0.4290 - acc: 0.8246 - val_loss: 0.3950 - val_acc: 0.8189\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4145 - acc: 0.8377 - val_loss: 0.3899 - val_acc: 0.8346\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 149us/step - loss: 0.4126 - acc: 0.8377 - val_loss: 0.3909 - val_acc: 0.8268\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4270 - acc: 0.8154 - val_loss: 0.3901 - val_acc: 0.8346\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4104 - acc: 0.8312 - val_loss: 0.3878 - val_acc: 0.8425\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 173us/step - loss: 0.4109 - acc: 0.8364 - val_loss: 0.3917 - val_acc: 0.8189\n",
      "processing fold # 6\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 497us/step - loss: 0.6307 - acc: 0.6636 - val_loss: 0.5252 - val_acc: 0.8425\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.5752 - acc: 0.7435 - val_loss: 0.4621 - val_acc: 0.8268\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.5299 - acc: 0.7814 - val_loss: 0.4404 - val_acc: 0.8268\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.5225 - acc: 0.7696 - val_loss: 0.4196 - val_acc: 0.8425\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.5058 - acc: 0.7840 - val_loss: 0.4132 - val_acc: 0.8583\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.5092 - acc: 0.7840 - val_loss: 0.4093 - val_acc: 0.8504\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4970 - acc: 0.7736 - val_loss: 0.4059 - val_acc: 0.8504\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4960 - acc: 0.7932 - val_loss: 0.4062 - val_acc: 0.8504\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4792 - acc: 0.7906 - val_loss: 0.4031 - val_acc: 0.8504\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4849 - acc: 0.7893 - val_loss: 0.3935 - val_acc: 0.8583\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4624 - acc: 0.8024 - val_loss: 0.4041 - val_acc: 0.8504\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4523 - acc: 0.8089 - val_loss: 0.3819 - val_acc: 0.8583\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 209us/step - loss: 0.4714 - acc: 0.7984 - val_loss: 0.3849 - val_acc: 0.8583\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 175us/step - loss: 0.4549 - acc: 0.7984 - val_loss: 0.3831 - val_acc: 0.8583\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4765 - acc: 0.7984 - val_loss: 0.3931 - val_acc: 0.8583\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 182us/step - loss: 0.4443 - acc: 0.8141 - val_loss: 0.3854 - val_acc: 0.8583\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 169us/step - loss: 0.4530 - acc: 0.8063 - val_loss: 0.3906 - val_acc: 0.8583\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4395 - acc: 0.8076 - val_loss: 0.3903 - val_acc: 0.8583\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4531 - acc: 0.8141 - val_loss: 0.3865 - val_acc: 0.8583\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 254us/step - loss: 0.4564 - acc: 0.8010 - val_loss: 0.3865 - val_acc: 0.8583\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 171us/step - loss: 0.4478 - acc: 0.7984 - val_loss: 0.3801 - val_acc: 0.8583\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 158us/step - loss: 0.4539 - acc: 0.8128 - val_loss: 0.3782 - val_acc: 0.8583\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4474 - acc: 0.8220 - val_loss: 0.3827 - val_acc: 0.8583\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4340 - acc: 0.8272 - val_loss: 0.3822 - val_acc: 0.8583\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 187us/step - loss: 0.4517 - acc: 0.8089 - val_loss: 0.3854 - val_acc: 0.8583\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 186us/step - loss: 0.4572 - acc: 0.8050 - val_loss: 0.3829 - val_acc: 0.8661\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 176us/step - loss: 0.4238 - acc: 0.8207 - val_loss: 0.3824 - val_acc: 0.8661\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4266 - acc: 0.8141 - val_loss: 0.3844 - val_acc: 0.8661\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4315 - acc: 0.8141 - val_loss: 0.3857 - val_acc: 0.8661\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4332 - acc: 0.8154 - val_loss: 0.3856 - val_acc: 0.8661\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4425 - acc: 0.8154 - val_loss: 0.3953 - val_acc: 0.8583\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4241 - acc: 0.8259 - val_loss: 0.3879 - val_acc: 0.8583\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4244 - acc: 0.8259 - val_loss: 0.3935 - val_acc: 0.8583\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4342 - acc: 0.8168 - val_loss: 0.3936 - val_acc: 0.8583\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4175 - acc: 0.8259 - val_loss: 0.3874 - val_acc: 0.8661\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4204 - acc: 0.8338 - val_loss: 0.3918 - val_acc: 0.8583\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4174 - acc: 0.8272 - val_loss: 0.3933 - val_acc: 0.8583\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 166us/step - loss: 0.4358 - acc: 0.8181 - val_loss: 0.3850 - val_acc: 0.8661\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 181us/step - loss: 0.4171 - acc: 0.8154 - val_loss: 0.3882 - val_acc: 0.8661\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4282 - acc: 0.8220 - val_loss: 0.3924 - val_acc: 0.8661\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 151us/step - loss: 0.4194 - acc: 0.8246 - val_loss: 0.3908 - val_acc: 0.8661\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 166us/step - loss: 0.4292 - acc: 0.8272 - val_loss: 0.3914 - val_acc: 0.8504\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 171us/step - loss: 0.4227 - acc: 0.8312 - val_loss: 0.3910 - val_acc: 0.8661\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4213 - acc: 0.8416 - val_loss: 0.3964 - val_acc: 0.8583\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4192 - acc: 0.8233 - val_loss: 0.3877 - val_acc: 0.8661\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4202 - acc: 0.8259 - val_loss: 0.3909 - val_acc: 0.8661\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 179us/step - loss: 0.4345 - acc: 0.8141 - val_loss: 0.3898 - val_acc: 0.8661\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 152us/step - loss: 0.4085 - acc: 0.8364 - val_loss: 0.3894 - val_acc: 0.8661\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4179 - acc: 0.8233 - val_loss: 0.3884 - val_acc: 0.8661\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 186us/step - loss: 0.4152 - acc: 0.8272 - val_loss: 0.3908 - val_acc: 0.8661\n"
     ]
    }
   ],
   "source": [
    "x_train = normalized_train_data.drop([\"Survived\"], axis=1).values\n",
    "y_train = normalized_train_data[\"Survived\"].values\n",
    "\n",
    "number_of_epochs = 50\n",
    "\n",
    "number_of_folds = 7\n",
    "number_of_samples = len(x_train) // number_of_folds\n",
    "\n",
    "all_histories = []\n",
    "for i in range(number_of_folds):\n",
    "    print(\"processing fold #\", i)\n",
    "    \n",
    "    partial_x_train = np.concatenate([x_train[:i*number_of_samples],\n",
    "                                          x_train[(i+1)*number_of_samples:]])\n",
    "    parital_y_train = np.concatenate([y_train[:i*number_of_samples],\n",
    "                                          y_train[(i+1)*number_of_samples:]])\n",
    "    \n",
    "    partial_x_validation = x_train[i*number_of_samples:(i+1)*number_of_samples]\n",
    "    partial_y_validation = y_train[i*number_of_samples:(i+1)*number_of_samples]\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation=\"relu\", input_shape=(x_train.shape[1],)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(RMSprop(lr=0.001),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"acc\"])\n",
    "\n",
    "    history = model.fit(partial_x_train,\n",
    "                        parital_y_train,\n",
    "                        epochs=number_of_epochs,\n",
    "                        batch_size=16,\n",
    "                        validation_data=[partial_x_validation,partial_y_validation])\n",
    "    all_histories.append(history.history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "loss_values = [np.mean([x[\"loss\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "val_loss_values = [np.mean([x[\"val_loss\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\", color=\"red\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "acc_values = [np.mean([x[\"acc\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "val_acc_values = [np.mean([x[\"val_acc\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "\n",
    "plt.plot(epochs, acc_values, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc_values, \"b\", label=\"Validation acc\", color=\"red\")\n",
    "plt.title(\"Training and validation acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived\n",
       "PassengerId          \n",
       "892                 0\n",
       "893                 0\n",
       "894                 0\n",
       "895                 0\n",
       "896                 1\n",
       "...               ...\n",
       "1300                1\n",
       "1302                1\n",
       "1305                0\n",
       "1308                0\n",
       "1309                1\n",
       "\n",
       "[418 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.predict(normalized_test_data.drop([\"Survived\"], axis=1))\n",
    "results = pd.DataFrame(results, columns=[\"Survived\"], index=normalized_test_data.index)\n",
    "\n",
    "results.loc[results.Survived < 0.5, [\"Survived\"]] = 0\n",
    "results.loc[results.Survived >= 0.5, [\"Survived\"]] = 1\n",
    "results = results.fillna(0)\n",
    "results.Survived = results.Survived.astype(int)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"../Output/my_prediction.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with genderr submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Example</th>\n",
       "      <th>MyPrediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1086</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1088</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1094</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1106</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1199</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1231</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1257</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Example  MyPrediction\n",
       "PassengerId                       \n",
       "893                1             0\n",
       "913                0             1\n",
       "925                1             0\n",
       "928                1             0\n",
       "929                1             0\n",
       "956                0             1\n",
       "964                1             0\n",
       "972                0             1\n",
       "980                1             0\n",
       "981                0             1\n",
       "990                1             0\n",
       "1017               1             0\n",
       "1023               0             1\n",
       "1024               1             0\n",
       "1030               1             0\n",
       "1032               1             0\n",
       "1045               1             0\n",
       "1049               1             0\n",
       "1051               1             0\n",
       "1053               0             1\n",
       "1061               1             0\n",
       "1080               1             0\n",
       "1086               0             1\n",
       "1088               0             1\n",
       "1093               0             1\n",
       "1094               0             1\n",
       "1106               1             0\n",
       "1172               1             0\n",
       "1199               0             1\n",
       "1201               1             0\n",
       "1231               0             1\n",
       "1257               1             0\n",
       "1268               1             0\n",
       "1284               0             1\n",
       "1301               1             0\n",
       "1304               1             0\n",
       "1309               0             1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare = pd.read_csv(\"../Dataset/gender_submission.csv\", index_col=0)\n",
    "compare = compare.rename(columns={\"Survived\": \"Example\"})\n",
    "compare = pd.concat([compare, results], axis=1)\n",
    "compare = compare.rename(columns={\"Survived\": \"MyPrediction\"})\n",
    "compare[compare.Example != compare.MyPrediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
