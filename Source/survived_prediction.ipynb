{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>...</th>\n",
       "      <th>SOTON/O.Q.</th>\n",
       "      <th>2.</th>\n",
       "      <th>STON/O</th>\n",
       "      <th>W./C.</th>\n",
       "      <th>CA.</th>\n",
       "      <th>A/5</th>\n",
       "      <th>SC/PARIS</th>\n",
       "      <th>2343</th>\n",
       "      <th>CA</th>\n",
       "      <th>A/5.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.274099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.274099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.344905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.344905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.034439</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass       Age  SibSp     Parch      Fare  female  \\\n",
       "PassengerId                                                                  \n",
       "1                 0.0     1.0  0.271174    0.2  0.000000  0.014151     0.0   \n",
       "2                 1.0     0.0  0.472229    0.2  0.000000  0.139136     1.0   \n",
       "3                 1.0     1.0  0.321438    0.0  0.000000  0.015469     1.0   \n",
       "4                 1.0     0.0  0.434531    0.2  0.000000  0.103644     1.0   \n",
       "5                 0.0     1.0  0.434531    0.0  0.000000  0.015713     0.0   \n",
       "...               ...     ...       ...    ...       ...       ...     ...   \n",
       "1300              NaN     1.0  0.274099    0.0  0.000000  0.015070     1.0   \n",
       "1302              NaN     1.0  0.274099    0.0  0.000000  0.015127     1.0   \n",
       "1305              NaN     1.0  0.344905    0.0  0.000000  0.015713     0.0   \n",
       "1308              NaN     1.0  0.344905    0.0  0.000000  0.015713     0.0   \n",
       "1309              NaN     1.0  0.034439    0.2  0.166667  0.043640     0.0   \n",
       "\n",
       "             male    C    Q  ...  SOTON/O.Q.   2.  STON/O  W./C.  CA.  A/5  \\\n",
       "PassengerId                  ...                                             \n",
       "1             1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  1.0   \n",
       "2             0.0  1.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "3             0.0  0.0  0.0  ...         0.0  1.0     1.0    0.0  0.0  0.0   \n",
       "4             0.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "5             1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "...           ...  ...  ...  ...         ...  ...     ...    ...  ...  ...   \n",
       "1300          0.0  0.0  1.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "1302          0.0  0.0  1.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "1305          1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "1308          1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "1309          1.0  1.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "\n",
       "             SC/PARIS  2343   CA  A/5.  \n",
       "PassengerId                             \n",
       "1                 0.0   0.0  0.0   0.0  \n",
       "2                 0.0   0.0  0.0   0.0  \n",
       "3                 0.0   0.0  0.0   0.0  \n",
       "4                 0.0   0.0  0.0   0.0  \n",
       "5                 0.0   0.0  0.0   0.0  \n",
       "...               ...   ...  ...   ...  \n",
       "1300              0.0   0.0  0.0   0.0  \n",
       "1302              0.0   0.0  0.0   0.0  \n",
       "1305              0.0   0.0  0.0   0.0  \n",
       "1308              0.0   0.0  0.0   0.0  \n",
       "1309              0.0   0.0  0.0   0.0  \n",
       "\n",
       "[1309 rows x 26 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from keras.optimizers import RMSprop\n",
    "seed(1)\n",
    "\n",
    "normalized_data = pd.read_csv(\"../Output/normalized_data_with_predicted_age.csv\", index_col=0)\n",
    "normalized_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>...</th>\n",
       "      <th>SOTON/O.Q.</th>\n",
       "      <th>2.</th>\n",
       "      <th>STON/O</th>\n",
       "      <th>W./C.</th>\n",
       "      <th>CA.</th>\n",
       "      <th>A/5</th>\n",
       "      <th>SC/PARIS</th>\n",
       "      <th>2343</th>\n",
       "      <th>CA</th>\n",
       "      <th>A/5.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.326684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.143992</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.135753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.344874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.344908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332308</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.045771</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass       Age  SibSp     Parch      Fare  female  \\\n",
       "PassengerId                                                                  \n",
       "1                 0.0     1.0  0.271174    0.2  0.000000  0.014151     0.0   \n",
       "2                 1.0     0.0  0.472229    0.2  0.000000  0.139136     1.0   \n",
       "3                 1.0     1.0  0.321438    0.0  0.000000  0.015469     1.0   \n",
       "4                 1.0     0.0  0.434531    0.2  0.000000  0.103644     1.0   \n",
       "5                 0.0     1.0  0.434531    0.0  0.000000  0.015713     0.0   \n",
       "...               ...     ...       ...    ...       ...       ...     ...   \n",
       "860               0.0     1.0  0.326684    0.0  0.000000  0.014110     0.0   \n",
       "864               0.0     1.0  0.143992    1.6  0.333333  0.135753     1.0   \n",
       "869               0.0     1.0  0.344874    0.0  0.000000  0.018543     0.0   \n",
       "879               0.0     1.0  0.344908    0.0  0.000000  0.015412     0.0   \n",
       "889               0.0     1.0  0.332308    0.2  0.333333  0.045771     1.0   \n",
       "\n",
       "             male    C    Q  ...  SOTON/O.Q.   2.  STON/O  W./C.  CA.  A/5  \\\n",
       "PassengerId                  ...                                             \n",
       "1             1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  1.0   \n",
       "2             0.0  1.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "3             0.0  0.0  0.0  ...         0.0  1.0     1.0    0.0  0.0  0.0   \n",
       "4             0.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "5             1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "...           ...  ...  ...  ...         ...  ...     ...    ...  ...  ...   \n",
       "860           1.0  1.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "864           0.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  1.0  0.0   \n",
       "869           1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "879           1.0  0.0  0.0  ...         0.0  0.0     0.0    0.0  0.0  0.0   \n",
       "889           0.0  0.0  0.0  ...         0.0  0.0     0.0    1.0  0.0  0.0   \n",
       "\n",
       "             SC/PARIS  2343   CA  A/5.  \n",
       "PassengerId                             \n",
       "1                 0.0   0.0  0.0   0.0  \n",
       "2                 0.0   0.0  0.0   0.0  \n",
       "3                 0.0   0.0  0.0   0.0  \n",
       "4                 0.0   0.0  0.0   0.0  \n",
       "5                 0.0   0.0  0.0   0.0  \n",
       "...               ...   ...  ...   ...  \n",
       "860               0.0   0.0  0.0   0.0  \n",
       "864               0.0   1.0  1.0   0.0  \n",
       "869               0.0   0.0  0.0   0.0  \n",
       "879               0.0   0.0  0.0   0.0  \n",
       "889               0.0   0.0  0.0   0.0  \n",
       "\n",
       "[891 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train_data = normalized_data[normalized_data.Survived.notna()]\n",
    "normalized_test_data = normalized_data[normalized_data.Survived.isna()]\n",
    "normalized_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 528us/step - loss: 0.6561 - acc: 0.6335 - val_loss: 0.5546 - val_acc: 0.7953\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.5640 - acc: 0.7605 - val_loss: 0.4841 - val_acc: 0.8110\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 146us/step - loss: 0.5233 - acc: 0.7762 - val_loss: 0.4644 - val_acc: 0.8031\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.5090 - acc: 0.7827 - val_loss: 0.4510 - val_acc: 0.8031\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.5162 - acc: 0.7853 - val_loss: 0.4431 - val_acc: 0.8110\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4887 - acc: 0.7932 - val_loss: 0.4527 - val_acc: 0.8031\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4747 - acc: 0.7971 - val_loss: 0.4484 - val_acc: 0.7953\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 146us/step - loss: 0.4832 - acc: 0.7906 - val_loss: 0.4367 - val_acc: 0.8268\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 207us/step - loss: 0.4684 - acc: 0.7958 - val_loss: 0.4252 - val_acc: 0.8268\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 186us/step - loss: 0.4709 - acc: 0.7893 - val_loss: 0.4263 - val_acc: 0.8189\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 157us/step - loss: 0.4596 - acc: 0.8037 - val_loss: 0.4355 - val_acc: 0.8189\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 158us/step - loss: 0.4547 - acc: 0.8050 - val_loss: 0.4398 - val_acc: 0.8031\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4565 - acc: 0.8076 - val_loss: 0.4358 - val_acc: 0.8031\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4697 - acc: 0.8037 - val_loss: 0.4327 - val_acc: 0.8031\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4602 - acc: 0.8154 - val_loss: 0.4410 - val_acc: 0.8189\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4584 - acc: 0.8050 - val_loss: 0.4318 - val_acc: 0.8110\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 208us/step - loss: 0.4484 - acc: 0.8154 - val_loss: 0.4357 - val_acc: 0.8031\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 187us/step - loss: 0.4671 - acc: 0.8050 - val_loss: 0.4382 - val_acc: 0.8031\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4449 - acc: 0.8233 - val_loss: 0.4404 - val_acc: 0.8110\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4517 - acc: 0.8168 - val_loss: 0.4361 - val_acc: 0.7953\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4389 - acc: 0.8181 - val_loss: 0.4340 - val_acc: 0.7953\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4423 - acc: 0.8246 - val_loss: 0.4324 - val_acc: 0.8110\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4404 - acc: 0.8141 - val_loss: 0.4375 - val_acc: 0.8346\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.4273 - acc: 0.8181 - val_loss: 0.4356 - val_acc: 0.8268\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 155us/step - loss: 0.4474 - acc: 0.8115 - val_loss: 0.4286 - val_acc: 0.8189\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4270 - acc: 0.8246 - val_loss: 0.4365 - val_acc: 0.8110\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4292 - acc: 0.8272 - val_loss: 0.4300 - val_acc: 0.8189\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4235 - acc: 0.8233 - val_loss: 0.4278 - val_acc: 0.8110\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4210 - acc: 0.8233 - val_loss: 0.4314 - val_acc: 0.8110\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4279 - acc: 0.8168 - val_loss: 0.4478 - val_acc: 0.8346\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4314 - acc: 0.8220 - val_loss: 0.4348 - val_acc: 0.8189\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4344 - acc: 0.8246 - val_loss: 0.4310 - val_acc: 0.8189\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4347 - acc: 0.8194 - val_loss: 0.4281 - val_acc: 0.8189\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4280 - acc: 0.8325 - val_loss: 0.4310 - val_acc: 0.8425\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 146us/step - loss: 0.4219 - acc: 0.8233 - val_loss: 0.4290 - val_acc: 0.8189\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4196 - acc: 0.8338 - val_loss: 0.4249 - val_acc: 0.8189\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4232 - acc: 0.8325 - val_loss: 0.4299 - val_acc: 0.8189\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 188us/step - loss: 0.4175 - acc: 0.8429 - val_loss: 0.4223 - val_acc: 0.8189\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 150us/step - loss: 0.4085 - acc: 0.8416 - val_loss: 0.4281 - val_acc: 0.8189\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.4100 - acc: 0.8351 - val_loss: 0.4278 - val_acc: 0.8189\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 178us/step - loss: 0.4264 - acc: 0.8246 - val_loss: 0.4234 - val_acc: 0.8268\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 149us/step - loss: 0.4136 - acc: 0.8403 - val_loss: 0.4367 - val_acc: 0.8346\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 170us/step - loss: 0.4219 - acc: 0.8181 - val_loss: 0.4245 - val_acc: 0.8346\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4028 - acc: 0.8298 - val_loss: 0.4164 - val_acc: 0.8268\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4026 - acc: 0.8469 - val_loss: 0.4226 - val_acc: 0.8346\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4114 - acc: 0.8442 - val_loss: 0.4221 - val_acc: 0.8110\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 175us/step - loss: 0.4018 - acc: 0.8416 - val_loss: 0.4204 - val_acc: 0.8346\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4061 - acc: 0.8390 - val_loss: 0.4234 - val_acc: 0.8189\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4352 - acc: 0.8312 - val_loss: 0.4307 - val_acc: 0.8346\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4236 - acc: 0.8325 - val_loss: 0.4170 - val_acc: 0.8346\n",
      "processing fold # 1\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 546us/step - loss: 0.6441 - acc: 0.6309 - val_loss: 0.5881 - val_acc: 0.7087\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.5305 - acc: 0.7631 - val_loss: 0.5302 - val_acc: 0.7717\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 189us/step - loss: 0.5043 - acc: 0.7853 - val_loss: 0.5191 - val_acc: 0.7638\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 156us/step - loss: 0.4905 - acc: 0.7919 - val_loss: 0.5144 - val_acc: 0.7638\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4845 - acc: 0.7997 - val_loss: 0.5145 - val_acc: 0.7480\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4627 - acc: 0.7997 - val_loss: 0.5160 - val_acc: 0.7402\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 158us/step - loss: 0.4528 - acc: 0.8181 - val_loss: 0.5137 - val_acc: 0.7717\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 161us/step - loss: 0.4683 - acc: 0.8063 - val_loss: 0.5146 - val_acc: 0.7480\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 149us/step - loss: 0.4657 - acc: 0.8010 - val_loss: 0.5136 - val_acc: 0.7402\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4434 - acc: 0.8141 - val_loss: 0.5189 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4495 - acc: 0.8076 - val_loss: 0.5099 - val_acc: 0.7717\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4506 - acc: 0.8089 - val_loss: 0.5088 - val_acc: 0.7559\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4414 - acc: 0.8102 - val_loss: 0.5117 - val_acc: 0.7795\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 155us/step - loss: 0.4349 - acc: 0.8207 - val_loss: 0.5160 - val_acc: 0.7795\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4485 - acc: 0.8115 - val_loss: 0.5100 - val_acc: 0.7795\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4254 - acc: 0.8259 - val_loss: 0.5129 - val_acc: 0.7638\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 174us/step - loss: 0.4150 - acc: 0.8246 - val_loss: 0.5185 - val_acc: 0.7795\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 184us/step - loss: 0.4090 - acc: 0.8325 - val_loss: 0.5312 - val_acc: 0.7795\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 159us/step - loss: 0.4339 - acc: 0.8220 - val_loss: 0.5274 - val_acc: 0.7795\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 156us/step - loss: 0.4411 - acc: 0.8220 - val_loss: 0.5223 - val_acc: 0.7795\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.4090 - acc: 0.8298 - val_loss: 0.5328 - val_acc: 0.7795\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 169us/step - loss: 0.4166 - acc: 0.8403 - val_loss: 0.5377 - val_acc: 0.7795\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 160us/step - loss: 0.4148 - acc: 0.8194 - val_loss: 0.5257 - val_acc: 0.7795\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4306 - acc: 0.8298 - val_loss: 0.5299 - val_acc: 0.7795\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4025 - acc: 0.8259 - val_loss: 0.5357 - val_acc: 0.7795\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4100 - acc: 0.8259 - val_loss: 0.5371 - val_acc: 0.7717\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4010 - acc: 0.8351 - val_loss: 0.5445 - val_acc: 0.7795\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4196 - acc: 0.8207 - val_loss: 0.5356 - val_acc: 0.7953\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4118 - acc: 0.8338 - val_loss: 0.5493 - val_acc: 0.7874\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4039 - acc: 0.8351 - val_loss: 0.5488 - val_acc: 0.7874\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 184us/step - loss: 0.4074 - acc: 0.8377 - val_loss: 0.5488 - val_acc: 0.7953\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4177 - acc: 0.8246 - val_loss: 0.5495 - val_acc: 0.7874\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4040 - acc: 0.8272 - val_loss: 0.5645 - val_acc: 0.7874\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.3987 - acc: 0.8364 - val_loss: 0.5575 - val_acc: 0.7953\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.3909 - acc: 0.8403 - val_loss: 0.5664 - val_acc: 0.7874\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4019 - acc: 0.8312 - val_loss: 0.5752 - val_acc: 0.7874\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.3839 - acc: 0.8390 - val_loss: 0.5864 - val_acc: 0.7953\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.3863 - acc: 0.8455 - val_loss: 0.5876 - val_acc: 0.7953\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 146us/step - loss: 0.3902 - acc: 0.8547 - val_loss: 0.5820 - val_acc: 0.8031\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.3857 - acc: 0.8495 - val_loss: 0.5943 - val_acc: 0.8110\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.3882 - acc: 0.8377 - val_loss: 0.6001 - val_acc: 0.8110\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.3946 - acc: 0.8351 - val_loss: 0.5887 - val_acc: 0.7953\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.3924 - acc: 0.8442 - val_loss: 0.5939 - val_acc: 0.8031\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.3823 - acc: 0.8377 - val_loss: 0.6125 - val_acc: 0.7953\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.3897 - acc: 0.8390 - val_loss: 0.6035 - val_acc: 0.8110\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.3966 - acc: 0.8403 - val_loss: 0.5966 - val_acc: 0.7953\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.3874 - acc: 0.8469 - val_loss: 0.5953 - val_acc: 0.7953\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.3876 - acc: 0.8390 - val_loss: 0.6158 - val_acc: 0.8031\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.3734 - acc: 0.8508 - val_loss: 0.6273 - val_acc: 0.8031\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.3991 - acc: 0.8298 - val_loss: 0.6082 - val_acc: 0.8031\n",
      "processing fold # 2\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 481us/step - loss: 0.6456 - acc: 0.6322 - val_loss: 0.5940 - val_acc: 0.7638\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.5479 - acc: 0.7565 - val_loss: 0.5061 - val_acc: 0.7953\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.5314 - acc: 0.7775 - val_loss: 0.4753 - val_acc: 0.7874\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4917 - acc: 0.7853 - val_loss: 0.4640 - val_acc: 0.7953\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4883 - acc: 0.7893 - val_loss: 0.4570 - val_acc: 0.7953\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4826 - acc: 0.7762 - val_loss: 0.4476 - val_acc: 0.8031\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4770 - acc: 0.7919 - val_loss: 0.4464 - val_acc: 0.8031\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4687 - acc: 0.7788 - val_loss: 0.4435 - val_acc: 0.8031\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4676 - acc: 0.8076 - val_loss: 0.4369 - val_acc: 0.8031\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4720 - acc: 0.8050 - val_loss: 0.4363 - val_acc: 0.8031\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4565 - acc: 0.7984 - val_loss: 0.4333 - val_acc: 0.8189\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4411 - acc: 0.8037 - val_loss: 0.4245 - val_acc: 0.8110\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4494 - acc: 0.7984 - val_loss: 0.4253 - val_acc: 0.8110\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4435 - acc: 0.8089 - val_loss: 0.4274 - val_acc: 0.8031\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4546 - acc: 0.8102 - val_loss: 0.4156 - val_acc: 0.8110\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4483 - acc: 0.8024 - val_loss: 0.4125 - val_acc: 0.8031\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4526 - acc: 0.8050 - val_loss: 0.4106 - val_acc: 0.8110\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4488 - acc: 0.8050 - val_loss: 0.4088 - val_acc: 0.8189\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.4495 - acc: 0.8063 - val_loss: 0.4048 - val_acc: 0.8346\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4349 - acc: 0.8272 - val_loss: 0.4012 - val_acc: 0.8268\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 126us/step - loss: 0.4365 - acc: 0.8233 - val_loss: 0.3985 - val_acc: 0.8346\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4433 - acc: 0.8181 - val_loss: 0.3992 - val_acc: 0.8268\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.4389 - acc: 0.8194 - val_loss: 0.3969 - val_acc: 0.8268\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4503 - acc: 0.8220 - val_loss: 0.3989 - val_acc: 0.8268\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4389 - acc: 0.8246 - val_loss: 0.3947 - val_acc: 0.8268\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4280 - acc: 0.8285 - val_loss: 0.3988 - val_acc: 0.8346\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 198us/step - loss: 0.4294 - acc: 0.8312 - val_loss: 0.3947 - val_acc: 0.8268\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 152us/step - loss: 0.4330 - acc: 0.8298 - val_loss: 0.3882 - val_acc: 0.8268\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 225us/step - loss: 0.4313 - acc: 0.8233 - val_loss: 0.3899 - val_acc: 0.8268\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 209us/step - loss: 0.4244 - acc: 0.8194 - val_loss: 0.3875 - val_acc: 0.8268\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4113 - acc: 0.8325 - val_loss: 0.3882 - val_acc: 0.8425\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 242us/step - loss: 0.4214 - acc: 0.8351 - val_loss: 0.3918 - val_acc: 0.8346\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 227us/step - loss: 0.4056 - acc: 0.8351 - val_loss: 0.3865 - val_acc: 0.8504\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 262us/step - loss: 0.4292 - acc: 0.8325 - val_loss: 0.3876 - val_acc: 0.8504\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 241us/step - loss: 0.4195 - acc: 0.8351 - val_loss: 0.3877 - val_acc: 0.8425\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4292 - acc: 0.8312 - val_loss: 0.3886 - val_acc: 0.8425\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.4218 - acc: 0.8312 - val_loss: 0.3853 - val_acc: 0.8346\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4045 - acc: 0.8351 - val_loss: 0.3822 - val_acc: 0.8346\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4254 - acc: 0.8207 - val_loss: 0.3876 - val_acc: 0.8346\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4142 - acc: 0.8364 - val_loss: 0.3821 - val_acc: 0.8504\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4215 - acc: 0.8403 - val_loss: 0.3842 - val_acc: 0.8425\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 150us/step - loss: 0.4234 - acc: 0.8351 - val_loss: 0.3889 - val_acc: 0.8346\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4290 - acc: 0.8338 - val_loss: 0.3885 - val_acc: 0.8346\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4201 - acc: 0.8364 - val_loss: 0.3872 - val_acc: 0.8425\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4008 - acc: 0.8429 - val_loss: 0.3830 - val_acc: 0.8425\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4256 - acc: 0.8377 - val_loss: 0.3799 - val_acc: 0.8425\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4209 - acc: 0.8351 - val_loss: 0.3831 - val_acc: 0.8425\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4117 - acc: 0.8351 - val_loss: 0.3819 - val_acc: 0.8425\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4172 - acc: 0.8377 - val_loss: 0.3821 - val_acc: 0.8189\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4146 - acc: 0.8364 - val_loss: 0.3837 - val_acc: 0.8346\n",
      "processing fold # 3\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 1s 684us/step - loss: 0.6109 - acc: 0.6453 - val_loss: 0.5836 - val_acc: 0.7402\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 162us/step - loss: 0.5479 - acc: 0.7369 - val_loss: 0.5537 - val_acc: 0.7480\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.5203 - acc: 0.7762 - val_loss: 0.5522 - val_acc: 0.7480\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.5174 - acc: 0.7814 - val_loss: 0.5529 - val_acc: 0.7480\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 213us/step - loss: 0.4975 - acc: 0.7866 - val_loss: 0.5359 - val_acc: 0.7559\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 149us/step - loss: 0.4678 - acc: 0.7945 - val_loss: 0.5371 - val_acc: 0.7559\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 149us/step - loss: 0.4817 - acc: 0.8010 - val_loss: 0.5233 - val_acc: 0.7559\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 161us/step - loss: 0.4624 - acc: 0.8115 - val_loss: 0.5152 - val_acc: 0.7559\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4916 - acc: 0.7984 - val_loss: 0.4986 - val_acc: 0.7638\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 178us/step - loss: 0.4629 - acc: 0.8141 - val_loss: 0.4936 - val_acc: 0.7638\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4582 - acc: 0.8102 - val_loss: 0.4980 - val_acc: 0.7717\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4566 - acc: 0.8154 - val_loss: 0.4840 - val_acc: 0.7795\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 178us/step - loss: 0.4564 - acc: 0.8115 - val_loss: 0.4747 - val_acc: 0.7874\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 155us/step - loss: 0.4420 - acc: 0.8102 - val_loss: 0.4775 - val_acc: 0.7795\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4653 - acc: 0.8102 - val_loss: 0.4737 - val_acc: 0.7874\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4325 - acc: 0.8154 - val_loss: 0.4761 - val_acc: 0.7874\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4381 - acc: 0.8102 - val_loss: 0.4658 - val_acc: 0.7874\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 185us/step - loss: 0.4460 - acc: 0.8128 - val_loss: 0.4678 - val_acc: 0.7874\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 187us/step - loss: 0.4509 - acc: 0.8115 - val_loss: 0.4696 - val_acc: 0.7874\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.4365 - acc: 0.8194 - val_loss: 0.4673 - val_acc: 0.7953\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4465 - acc: 0.8168 - val_loss: 0.4626 - val_acc: 0.7795\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4318 - acc: 0.8063 - val_loss: 0.4652 - val_acc: 0.7795\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 163us/step - loss: 0.4369 - acc: 0.8102 - val_loss: 0.4627 - val_acc: 0.7874\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4256 - acc: 0.8207 - val_loss: 0.4560 - val_acc: 0.7795\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 153us/step - loss: 0.4362 - acc: 0.8207 - val_loss: 0.4607 - val_acc: 0.7795\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4140 - acc: 0.8390 - val_loss: 0.4735 - val_acc: 0.7795\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 172us/step - loss: 0.4358 - acc: 0.8154 - val_loss: 0.4651 - val_acc: 0.7795\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4324 - acc: 0.8207 - val_loss: 0.4566 - val_acc: 0.8031\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.4399 - acc: 0.8233 - val_loss: 0.4633 - val_acc: 0.7953\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4446 - acc: 0.8141 - val_loss: 0.4608 - val_acc: 0.7795\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 126us/step - loss: 0.4242 - acc: 0.8312 - val_loss: 0.4552 - val_acc: 0.8031\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4269 - acc: 0.8181 - val_loss: 0.4614 - val_acc: 0.7874\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4190 - acc: 0.8285 - val_loss: 0.4612 - val_acc: 0.8031\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4268 - acc: 0.8181 - val_loss: 0.4634 - val_acc: 0.7953\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4302 - acc: 0.8207 - val_loss: 0.4617 - val_acc: 0.7953\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4328 - acc: 0.8220 - val_loss: 0.4561 - val_acc: 0.7953\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4313 - acc: 0.8298 - val_loss: 0.4591 - val_acc: 0.7874\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 167us/step - loss: 0.4297 - acc: 0.8298 - val_loss: 0.4632 - val_acc: 0.7795\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4195 - acc: 0.8429 - val_loss: 0.4676 - val_acc: 0.7874\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4270 - acc: 0.8272 - val_loss: 0.4686 - val_acc: 0.7874\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4210 - acc: 0.8272 - val_loss: 0.4611 - val_acc: 0.7874\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4215 - acc: 0.8351 - val_loss: 0.4685 - val_acc: 0.7874\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4149 - acc: 0.8285 - val_loss: 0.4637 - val_acc: 0.7953\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4248 - acc: 0.8246 - val_loss: 0.4693 - val_acc: 0.7953\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4238 - acc: 0.8246 - val_loss: 0.4676 - val_acc: 0.7874\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4200 - acc: 0.8272 - val_loss: 0.4688 - val_acc: 0.7874\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4279 - acc: 0.8325 - val_loss: 0.4613 - val_acc: 0.7953\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4153 - acc: 0.8233 - val_loss: 0.4664 - val_acc: 0.7953\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4179 - acc: 0.8364 - val_loss: 0.4614 - val_acc: 0.8031\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4130 - acc: 0.8377 - val_loss: 0.4612 - val_acc: 0.8031\n",
      "processing fold # 4\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 474us/step - loss: 0.6253 - acc: 0.6806 - val_loss: 0.5440 - val_acc: 0.7638\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.5607 - acc: 0.7369 - val_loss: 0.4868 - val_acc: 0.7795\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.5336 - acc: 0.7749 - val_loss: 0.4638 - val_acc: 0.7795\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 180us/step - loss: 0.5116 - acc: 0.7893 - val_loss: 0.4566 - val_acc: 0.7795\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.5147 - acc: 0.7840 - val_loss: 0.4493 - val_acc: 0.7953\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4997 - acc: 0.7866 - val_loss: 0.4443 - val_acc: 0.8031\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4960 - acc: 0.7827 - val_loss: 0.4397 - val_acc: 0.8031\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4906 - acc: 0.7958 - val_loss: 0.4424 - val_acc: 0.7953\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4839 - acc: 0.7814 - val_loss: 0.4392 - val_acc: 0.8031\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4590 - acc: 0.8233 - val_loss: 0.4334 - val_acc: 0.8031\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4572 - acc: 0.8024 - val_loss: 0.4340 - val_acc: 0.8031\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 106us/step - loss: 0.4630 - acc: 0.8076 - val_loss: 0.4279 - val_acc: 0.8031\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4631 - acc: 0.8168 - val_loss: 0.4319 - val_acc: 0.8031\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4534 - acc: 0.8115 - val_loss: 0.4195 - val_acc: 0.8268\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4605 - acc: 0.8194 - val_loss: 0.4189 - val_acc: 0.8110\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4588 - acc: 0.8154 - val_loss: 0.4212 - val_acc: 0.8268\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4521 - acc: 0.8168 - val_loss: 0.4191 - val_acc: 0.8110\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4509 - acc: 0.8168 - val_loss: 0.4219 - val_acc: 0.7953\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4451 - acc: 0.8194 - val_loss: 0.4212 - val_acc: 0.8110\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4396 - acc: 0.8128 - val_loss: 0.4151 - val_acc: 0.8268\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4400 - acc: 0.8194 - val_loss: 0.4158 - val_acc: 0.8031\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 170us/step - loss: 0.4533 - acc: 0.8207 - val_loss: 0.4184 - val_acc: 0.8110\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 185us/step - loss: 0.4362 - acc: 0.8351 - val_loss: 0.4196 - val_acc: 0.7953\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4384 - acc: 0.8181 - val_loss: 0.4222 - val_acc: 0.8031\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4369 - acc: 0.8285 - val_loss: 0.4220 - val_acc: 0.8031\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4110 - acc: 0.8312 - val_loss: 0.4270 - val_acc: 0.7953\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4314 - acc: 0.8403 - val_loss: 0.4290 - val_acc: 0.8031\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4199 - acc: 0.8338 - val_loss: 0.4284 - val_acc: 0.8031\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4465 - acc: 0.8220 - val_loss: 0.4306 - val_acc: 0.7953\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4255 - acc: 0.8338 - val_loss: 0.4252 - val_acc: 0.7953\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4253 - acc: 0.8246 - val_loss: 0.4238 - val_acc: 0.8110\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4275 - acc: 0.8338 - val_loss: 0.4260 - val_acc: 0.8268\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4266 - acc: 0.8325 - val_loss: 0.4282 - val_acc: 0.8110\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4382 - acc: 0.8246 - val_loss: 0.4311 - val_acc: 0.8031\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4163 - acc: 0.8312 - val_loss: 0.4355 - val_acc: 0.7953\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4204 - acc: 0.8285 - val_loss: 0.4337 - val_acc: 0.7953\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4149 - acc: 0.8298 - val_loss: 0.4394 - val_acc: 0.8031\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4322 - acc: 0.8325 - val_loss: 0.4420 - val_acc: 0.7953\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4322 - acc: 0.8390 - val_loss: 0.4355 - val_acc: 0.8031\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4109 - acc: 0.8325 - val_loss: 0.4393 - val_acc: 0.8031\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 116us/step - loss: 0.4134 - acc: 0.8416 - val_loss: 0.4341 - val_acc: 0.8031\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4240 - acc: 0.8325 - val_loss: 0.4356 - val_acc: 0.8110\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4265 - acc: 0.8351 - val_loss: 0.4384 - val_acc: 0.8031\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4108 - acc: 0.8442 - val_loss: 0.4386 - val_acc: 0.8031\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4135 - acc: 0.8377 - val_loss: 0.4361 - val_acc: 0.8110\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4133 - acc: 0.8325 - val_loss: 0.4404 - val_acc: 0.8110\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4180 - acc: 0.8351 - val_loss: 0.4459 - val_acc: 0.8031\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4155 - acc: 0.8377 - val_loss: 0.4504 - val_acc: 0.8031\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 107us/step - loss: 0.4109 - acc: 0.8351 - val_loss: 0.4471 - val_acc: 0.8031\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4191 - acc: 0.8325 - val_loss: 0.4517 - val_acc: 0.8031\n",
      "processing fold # 5\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 478us/step - loss: 0.6194 - acc: 0.6584 - val_loss: 0.5549 - val_acc: 0.7559\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 107us/step - loss: 0.5657 - acc: 0.7408 - val_loss: 0.5086 - val_acc: 0.7874\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.5327 - acc: 0.7723 - val_loss: 0.4907 - val_acc: 0.7795\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.5080 - acc: 0.7696 - val_loss: 0.4792 - val_acc: 0.7795\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4920 - acc: 0.7827 - val_loss: 0.4676 - val_acc: 0.7874\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4971 - acc: 0.7866 - val_loss: 0.4598 - val_acc: 0.7953\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4798 - acc: 0.7814 - val_loss: 0.4531 - val_acc: 0.7953\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4754 - acc: 0.7997 - val_loss: 0.4471 - val_acc: 0.8110\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4770 - acc: 0.7880 - val_loss: 0.4443 - val_acc: 0.8189\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 106us/step - loss: 0.4863 - acc: 0.7893 - val_loss: 0.4380 - val_acc: 0.8189\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4588 - acc: 0.7997 - val_loss: 0.4318 - val_acc: 0.8189\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4722 - acc: 0.7919 - val_loss: 0.4331 - val_acc: 0.8189\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4574 - acc: 0.8050 - val_loss: 0.4246 - val_acc: 0.8268\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4545 - acc: 0.8024 - val_loss: 0.4205 - val_acc: 0.8268\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4594 - acc: 0.8089 - val_loss: 0.4222 - val_acc: 0.8189\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4488 - acc: 0.8050 - val_loss: 0.4151 - val_acc: 0.8189\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 109us/step - loss: 0.4439 - acc: 0.8168 - val_loss: 0.4130 - val_acc: 0.8189\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 107us/step - loss: 0.4295 - acc: 0.8181 - val_loss: 0.4111 - val_acc: 0.8189\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4363 - acc: 0.8168 - val_loss: 0.4090 - val_acc: 0.8268\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 108us/step - loss: 0.4389 - acc: 0.8207 - val_loss: 0.4159 - val_acc: 0.8189\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4420 - acc: 0.8207 - val_loss: 0.4061 - val_acc: 0.8425\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4460 - acc: 0.8102 - val_loss: 0.4124 - val_acc: 0.8268\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4129 - acc: 0.8181 - val_loss: 0.4027 - val_acc: 0.8425\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 107us/step - loss: 0.4351 - acc: 0.8298 - val_loss: 0.4021 - val_acc: 0.8425\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4417 - acc: 0.8102 - val_loss: 0.4059 - val_acc: 0.8268\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4349 - acc: 0.8194 - val_loss: 0.4002 - val_acc: 0.8504\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4356 - acc: 0.8220 - val_loss: 0.4026 - val_acc: 0.8268\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4383 - acc: 0.8220 - val_loss: 0.3999 - val_acc: 0.8346\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4360 - acc: 0.8298 - val_loss: 0.4099 - val_acc: 0.8346\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4437 - acc: 0.8141 - val_loss: 0.4022 - val_acc: 0.8268\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4316 - acc: 0.8246 - val_loss: 0.3971 - val_acc: 0.8346\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4392 - acc: 0.8220 - val_loss: 0.3967 - val_acc: 0.8504\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4123 - acc: 0.8298 - val_loss: 0.3997 - val_acc: 0.8504\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4301 - acc: 0.8325 - val_loss: 0.3974 - val_acc: 0.8425\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4293 - acc: 0.8272 - val_loss: 0.3942 - val_acc: 0.8425\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 161us/step - loss: 0.4237 - acc: 0.8285 - val_loss: 0.3975 - val_acc: 0.8346\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4147 - acc: 0.8338 - val_loss: 0.3957 - val_acc: 0.8583\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4394 - acc: 0.8298 - val_loss: 0.3982 - val_acc: 0.8346\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4183 - acc: 0.8272 - val_loss: 0.3940 - val_acc: 0.8504\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4010 - acc: 0.8338 - val_loss: 0.3924 - val_acc: 0.8346\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4275 - acc: 0.8259 - val_loss: 0.3887 - val_acc: 0.8583\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4309 - acc: 0.8181 - val_loss: 0.3922 - val_acc: 0.8504\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4180 - acc: 0.8325 - val_loss: 0.3876 - val_acc: 0.8583\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4129 - acc: 0.8416 - val_loss: 0.3862 - val_acc: 0.8583\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4276 - acc: 0.8312 - val_loss: 0.3952 - val_acc: 0.8189\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4153 - acc: 0.8390 - val_loss: 0.3899 - val_acc: 0.8268\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4113 - acc: 0.8416 - val_loss: 0.3905 - val_acc: 0.8189\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4253 - acc: 0.8194 - val_loss: 0.3899 - val_acc: 0.8425\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4100 - acc: 0.8338 - val_loss: 0.3872 - val_acc: 0.8425\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4102 - acc: 0.8312 - val_loss: 0.3906 - val_acc: 0.8189\n",
      "processing fold # 6\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 613us/step - loss: 0.6305 - acc: 0.6636 - val_loss: 0.5252 - val_acc: 0.8425\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 108us/step - loss: 0.5749 - acc: 0.7448 - val_loss: 0.4621 - val_acc: 0.8268\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.5298 - acc: 0.7801 - val_loss: 0.4406 - val_acc: 0.8268\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.5225 - acc: 0.7696 - val_loss: 0.4197 - val_acc: 0.8425\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.5057 - acc: 0.7814 - val_loss: 0.4134 - val_acc: 0.8425\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.5096 - acc: 0.7853 - val_loss: 0.4096 - val_acc: 0.8504\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4973 - acc: 0.7736 - val_loss: 0.4056 - val_acc: 0.8504\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4960 - acc: 0.7919 - val_loss: 0.4063 - val_acc: 0.8504\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4796 - acc: 0.7919 - val_loss: 0.4033 - val_acc: 0.8504\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4855 - acc: 0.7866 - val_loss: 0.3940 - val_acc: 0.8583\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4621 - acc: 0.7997 - val_loss: 0.4039 - val_acc: 0.8504\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4527 - acc: 0.8115 - val_loss: 0.3817 - val_acc: 0.8583\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4718 - acc: 0.7984 - val_loss: 0.3854 - val_acc: 0.8583\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4539 - acc: 0.8010 - val_loss: 0.3823 - val_acc: 0.8583\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4772 - acc: 0.7997 - val_loss: 0.3930 - val_acc: 0.8583\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4434 - acc: 0.8102 - val_loss: 0.3845 - val_acc: 0.8583\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4527 - acc: 0.8063 - val_loss: 0.3891 - val_acc: 0.8583\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4389 - acc: 0.8089 - val_loss: 0.3897 - val_acc: 0.8583\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4537 - acc: 0.8194 - val_loss: 0.3871 - val_acc: 0.8583\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4572 - acc: 0.8037 - val_loss: 0.3865 - val_acc: 0.8583\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4482 - acc: 0.7945 - val_loss: 0.3804 - val_acc: 0.8583\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4538 - acc: 0.8102 - val_loss: 0.3777 - val_acc: 0.8583\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4465 - acc: 0.8207 - val_loss: 0.3825 - val_acc: 0.8583\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4338 - acc: 0.8259 - val_loss: 0.3824 - val_acc: 0.8583\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4515 - acc: 0.8050 - val_loss: 0.3867 - val_acc: 0.8661\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4575 - acc: 0.8076 - val_loss: 0.3839 - val_acc: 0.8661\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4245 - acc: 0.8207 - val_loss: 0.3814 - val_acc: 0.8661\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4255 - acc: 0.8141 - val_loss: 0.3838 - val_acc: 0.8661\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4314 - acc: 0.8128 - val_loss: 0.3852 - val_acc: 0.8661\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4339 - acc: 0.8141 - val_loss: 0.3864 - val_acc: 0.8583\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4437 - acc: 0.8102 - val_loss: 0.3970 - val_acc: 0.8583\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4259 - acc: 0.8272 - val_loss: 0.3887 - val_acc: 0.8583\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4241 - acc: 0.8233 - val_loss: 0.3944 - val_acc: 0.8583\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4352 - acc: 0.8168 - val_loss: 0.3931 - val_acc: 0.8583\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4190 - acc: 0.8207 - val_loss: 0.3888 - val_acc: 0.8661\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4211 - acc: 0.8312 - val_loss: 0.3930 - val_acc: 0.8583\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4190 - acc: 0.8233 - val_loss: 0.3944 - val_acc: 0.8583\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4370 - acc: 0.8154 - val_loss: 0.3856 - val_acc: 0.8504\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4179 - acc: 0.8181 - val_loss: 0.3887 - val_acc: 0.8661\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 108us/step - loss: 0.4265 - acc: 0.8298 - val_loss: 0.3932 - val_acc: 0.8661\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4195 - acc: 0.8233 - val_loss: 0.3909 - val_acc: 0.8661\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4293 - acc: 0.8259 - val_loss: 0.3906 - val_acc: 0.8425\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4231 - acc: 0.8298 - val_loss: 0.3911 - val_acc: 0.8583\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 109us/step - loss: 0.4229 - acc: 0.8390 - val_loss: 0.3976 - val_acc: 0.8583\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4215 - acc: 0.8233 - val_loss: 0.3888 - val_acc: 0.8504\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4227 - acc: 0.8246 - val_loss: 0.3912 - val_acc: 0.8583\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4344 - acc: 0.8207 - val_loss: 0.3892 - val_acc: 0.8661\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4078 - acc: 0.8377 - val_loss: 0.3902 - val_acc: 0.8661\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4197 - acc: 0.8246 - val_loss: 0.3890 - val_acc: 0.8583\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4168 - acc: 0.8246 - val_loss: 0.3907 - val_acc: 0.8583\n"
     ]
    }
   ],
   "source": [
    "x_train = normalized_train_data.drop([\"Survived\"], axis=1).values\n",
    "y_train = normalized_train_data[\"Survived\"].values\n",
    "\n",
    "number_of_epochs = 50\n",
    "\n",
    "number_of_folds = 7\n",
    "number_of_samples = len(x_train) // number_of_folds\n",
    "\n",
    "all_histories = []\n",
    "for i in range(number_of_folds):\n",
    "    print(\"processing fold #\", i)\n",
    "    \n",
    "    partial_x_train = np.concatenate([x_train[:i*number_of_samples],\n",
    "                                          x_train[(i+1)*number_of_samples:]])\n",
    "    parital_y_train = np.concatenate([y_train[:i*number_of_samples],\n",
    "                                          y_train[(i+1)*number_of_samples:]])\n",
    "    \n",
    "    partial_x_validation = x_train[i*number_of_samples:(i+1)*number_of_samples]\n",
    "    partial_y_validation = y_train[i*number_of_samples:(i+1)*number_of_samples]\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation=\"relu\", input_shape=(x_train.shape[1],)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(RMSprop(lr=0.001),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"acc\"])\n",
    "\n",
    "    history = model.fit(partial_x_train,\n",
    "                        parital_y_train,\n",
    "                        epochs=number_of_epochs,\n",
    "                        batch_size=16,\n",
    "                        validation_data=[partial_x_validation,partial_y_validation])\n",
    "    all_histories.append(history.history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "loss_values = [np.mean([x[\"loss\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "val_loss_values = [np.mean([x[\"val_loss\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\", color=\"red\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "acc_values = [np.mean([x[\"acc\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "val_acc_values = [np.mean([x[\"val_acc\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "\n",
    "plt.plot(epochs, acc_values, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc_values, \"b\", label=\"Validation acc\", color=\"red\")\n",
    "plt.title(\"Training and validation acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived\n",
       "PassengerId          \n",
       "892                 0\n",
       "893                 0\n",
       "894                 0\n",
       "895                 0\n",
       "896                 1\n",
       "...               ...\n",
       "1300                1\n",
       "1302                1\n",
       "1305                0\n",
       "1308                0\n",
       "1309                1\n",
       "\n",
       "[418 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.predict(normalized_test_data.drop([\"Survived\"], axis=1))\n",
    "results = pd.DataFrame(results, columns=[\"Survived\"], index=normalized_test_data.index)\n",
    "\n",
    "results.loc[results.Survived < 0.5, [\"Survived\"]] = 0\n",
    "results.loc[results.Survived >= 0.5, [\"Survived\"]] = 1\n",
    "results = results.fillna(0)\n",
    "results.Survived = results.Survived.astype(int)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"../Output/my_prediction.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with genderr submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Example</th>\n",
       "      <th>MyPrediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1086</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1088</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1094</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1106</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1199</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1231</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1257</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Example  MyPrediction\n",
       "PassengerId                       \n",
       "893                1             0\n",
       "913                0             1\n",
       "925                1             0\n",
       "928                1             0\n",
       "929                1             0\n",
       "956                0             1\n",
       "964                1             0\n",
       "972                0             1\n",
       "980                1             0\n",
       "981                0             1\n",
       "990                1             0\n",
       "1023               0             1\n",
       "1024               1             0\n",
       "1030               1             0\n",
       "1032               1             0\n",
       "1045               1             0\n",
       "1049               1             0\n",
       "1051               1             0\n",
       "1053               0             1\n",
       "1061               1             0\n",
       "1080               1             0\n",
       "1086               0             1\n",
       "1088               0             1\n",
       "1093               0             1\n",
       "1094               0             1\n",
       "1106               1             0\n",
       "1172               1             0\n",
       "1199               0             1\n",
       "1201               1             0\n",
       "1231               0             1\n",
       "1257               1             0\n",
       "1268               1             0\n",
       "1284               0             1\n",
       "1301               1             0\n",
       "1304               1             0\n",
       "1309               0             1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare = pd.read_csv(\"../Dataset/gender_submission.csv\", index_col=0)\n",
    "compare = compare.rename(columns={\"Survived\": \"Example\"})\n",
    "compare = pd.concat([compare, results], axis=1)\n",
    "compare = compare.rename(columns={\"Survived\": \"MyPrediction\"})\n",
    "compare[compare.Example != compare.MyPrediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
