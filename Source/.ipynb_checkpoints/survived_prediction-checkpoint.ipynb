{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>...</th>\n",
       "      <th>SOTON/O.Q.</th>\n",
       "      <th>2.</th>\n",
       "      <th>W./C.</th>\n",
       "      <th>STON/O</th>\n",
       "      <th>CA.</th>\n",
       "      <th>A/5</th>\n",
       "      <th>SC/PARIS</th>\n",
       "      <th>2343</th>\n",
       "      <th>CA</th>\n",
       "      <th>A/5.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.260001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.259994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.360627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.360627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017290</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass       Age  SibSp     Parch      Fare  female  \\\n",
       "PassengerId                                                                  \n",
       "1                 0.0     1.0  0.271174    0.2  0.000000  0.014151     0.0   \n",
       "2                 1.0     0.0  0.472229    0.2  0.000000  0.139136     1.0   \n",
       "3                 1.0     1.0  0.321438    0.0  0.000000  0.015469     1.0   \n",
       "4                 1.0     0.0  0.434531    0.2  0.000000  0.103644     1.0   \n",
       "5                 0.0     1.0  0.434531    0.0  0.000000  0.015713     0.0   \n",
       "...               ...     ...       ...    ...       ...       ...     ...   \n",
       "1300              NaN     1.0  0.260001    0.0  0.000000  0.015070     1.0   \n",
       "1302              NaN     1.0  0.259994    0.0  0.000000  0.015127     1.0   \n",
       "1305              NaN     1.0  0.360627    0.0  0.000000  0.015713     0.0   \n",
       "1308              NaN     1.0  0.360627    0.0  0.000000  0.015713     0.0   \n",
       "1309              NaN     1.0  0.017290    0.2  0.166667  0.043640     0.0   \n",
       "\n",
       "             male    C    Q  ...  SOTON/O.Q.   2.  W./C.  STON/O  CA.  A/5  \\\n",
       "PassengerId                  ...                                             \n",
       "1             1.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  1.0   \n",
       "2             0.0  1.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "3             0.0  0.0  0.0  ...         0.0  1.0    0.0     1.0  0.0  0.0   \n",
       "4             0.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "5             1.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "...           ...  ...  ...  ...         ...  ...    ...     ...  ...  ...   \n",
       "1300          0.0  0.0  1.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "1302          0.0  0.0  1.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "1305          1.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "1308          1.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "1309          1.0  1.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "\n",
       "             SC/PARIS  2343   CA  A/5.  \n",
       "PassengerId                             \n",
       "1                 0.0   0.0  0.0   0.0  \n",
       "2                 0.0   0.0  0.0   0.0  \n",
       "3                 0.0   0.0  0.0   0.0  \n",
       "4                 0.0   0.0  0.0   0.0  \n",
       "5                 0.0   0.0  0.0   0.0  \n",
       "...               ...   ...  ...   ...  \n",
       "1300              0.0   0.0  0.0   0.0  \n",
       "1302              0.0   0.0  0.0   0.0  \n",
       "1305              0.0   0.0  0.0   0.0  \n",
       "1308              0.0   0.0  0.0   0.0  \n",
       "1309              0.0   0.0  0.0   0.0  \n",
       "\n",
       "[1309 rows x 26 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from keras.optimizers import RMSprop\n",
    "seed(1)\n",
    "\n",
    "normalized_data = pd.read_csv(\"../Output/normalized_data_with_predicted_age.csv\", index_col=0)\n",
    "normalized_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>...</th>\n",
       "      <th>SOTON/O.Q.</th>\n",
       "      <th>2.</th>\n",
       "      <th>W./C.</th>\n",
       "      <th>STON/O</th>\n",
       "      <th>CA.</th>\n",
       "      <th>A/5</th>\n",
       "      <th>SC/PARIS</th>\n",
       "      <th>2343</th>\n",
       "      <th>CA</th>\n",
       "      <th>A/5.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.267828</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.135753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.360791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.360609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.274228</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.045771</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass       Age  SibSp     Parch      Fare  female  \\\n",
       "PassengerId                                                                  \n",
       "1                 0.0     1.0  0.271174    0.2  0.000000  0.014151     0.0   \n",
       "2                 1.0     0.0  0.472229    0.2  0.000000  0.139136     1.0   \n",
       "3                 1.0     1.0  0.321438    0.0  0.000000  0.015469     1.0   \n",
       "4                 1.0     0.0  0.434531    0.2  0.000000  0.103644     1.0   \n",
       "5                 0.0     1.0  0.434531    0.0  0.000000  0.015713     0.0   \n",
       "...               ...     ...       ...    ...       ...       ...     ...   \n",
       "860               0.0     1.0  0.331168    0.0  0.000000  0.014110     0.0   \n",
       "864               0.0     1.0  0.267828    1.6  0.333333  0.135753     1.0   \n",
       "869               0.0     1.0  0.360791    0.0  0.000000  0.018543     0.0   \n",
       "879               0.0     1.0  0.360609    0.0  0.000000  0.015412     0.0   \n",
       "889               0.0     1.0  0.274228    0.2  0.333333  0.045771     1.0   \n",
       "\n",
       "             male    C    Q  ...  SOTON/O.Q.   2.  W./C.  STON/O  CA.  A/5  \\\n",
       "PassengerId                  ...                                             \n",
       "1             1.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  1.0   \n",
       "2             0.0  1.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "3             0.0  0.0  0.0  ...         0.0  1.0    0.0     1.0  0.0  0.0   \n",
       "4             0.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "5             1.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "...           ...  ...  ...  ...         ...  ...    ...     ...  ...  ...   \n",
       "860           1.0  1.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "864           0.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  1.0  0.0   \n",
       "869           1.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "879           1.0  0.0  0.0  ...         0.0  0.0    0.0     0.0  0.0  0.0   \n",
       "889           0.0  0.0  0.0  ...         0.0  0.0    1.0     0.0  0.0  0.0   \n",
       "\n",
       "             SC/PARIS  2343   CA  A/5.  \n",
       "PassengerId                             \n",
       "1                 0.0   0.0  0.0   0.0  \n",
       "2                 0.0   0.0  0.0   0.0  \n",
       "3                 0.0   0.0  0.0   0.0  \n",
       "4                 0.0   0.0  0.0   0.0  \n",
       "5                 0.0   0.0  0.0   0.0  \n",
       "...               ...   ...  ...   ...  \n",
       "860               0.0   0.0  0.0   0.0  \n",
       "864               0.0   1.0  1.0   0.0  \n",
       "869               0.0   0.0  0.0   0.0  \n",
       "879               0.0   0.0  0.0   0.0  \n",
       "889               0.0   0.0  0.0   0.0  \n",
       "\n",
       "[891 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train_data = normalized_data[normalized_data.Survived.notna()]\n",
    "normalized_test_data = normalized_data[normalized_data.Survived.isna()]\n",
    "normalized_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 532us/step - loss: 0.6537 - acc: 0.6309 - val_loss: 0.5497 - val_acc: 0.8031\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.5625 - acc: 0.7723 - val_loss: 0.4794 - val_acc: 0.8110\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.5243 - acc: 0.7749 - val_loss: 0.4606 - val_acc: 0.8031\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.5080 - acc: 0.7840 - val_loss: 0.4467 - val_acc: 0.8031\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.5162 - acc: 0.7866 - val_loss: 0.4411 - val_acc: 0.8110\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4896 - acc: 0.7958 - val_loss: 0.4511 - val_acc: 0.7953\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.4730 - acc: 0.7971 - val_loss: 0.4431 - val_acc: 0.8110\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4812 - acc: 0.7958 - val_loss: 0.4340 - val_acc: 0.8268\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4674 - acc: 0.7945 - val_loss: 0.4229 - val_acc: 0.8346\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 154us/step - loss: 0.4689 - acc: 0.7840 - val_loss: 0.4231 - val_acc: 0.8189\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 159us/step - loss: 0.4558 - acc: 0.8076 - val_loss: 0.4317 - val_acc: 0.8268\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4547 - acc: 0.8037 - val_loss: 0.4332 - val_acc: 0.8110\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.4562 - acc: 0.8010 - val_loss: 0.4309 - val_acc: 0.8110\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.4696 - acc: 0.8050 - val_loss: 0.4286 - val_acc: 0.8268\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4587 - acc: 0.8102 - val_loss: 0.4382 - val_acc: 0.8268\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.4577 - acc: 0.8076 - val_loss: 0.4259 - val_acc: 0.8268\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4482 - acc: 0.8194 - val_loss: 0.4303 - val_acc: 0.8189\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4681 - acc: 0.7971 - val_loss: 0.4280 - val_acc: 0.8268\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4449 - acc: 0.8233 - val_loss: 0.4328 - val_acc: 0.8268\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.4549 - acc: 0.8076 - val_loss: 0.4311 - val_acc: 0.8189\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 175us/step - loss: 0.4359 - acc: 0.8194 - val_loss: 0.4268 - val_acc: 0.8189\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 160us/step - loss: 0.4445 - acc: 0.8141 - val_loss: 0.4247 - val_acc: 0.8189\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 174us/step - loss: 0.4454 - acc: 0.8063 - val_loss: 0.4320 - val_acc: 0.8268\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 159us/step - loss: 0.4341 - acc: 0.8141 - val_loss: 0.4302 - val_acc: 0.8346\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4506 - acc: 0.8141 - val_loss: 0.4230 - val_acc: 0.8189\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4273 - acc: 0.8168 - val_loss: 0.4272 - val_acc: 0.8346\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4328 - acc: 0.8181 - val_loss: 0.4244 - val_acc: 0.8189\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 154us/step - loss: 0.4250 - acc: 0.8141 - val_loss: 0.4247 - val_acc: 0.8268\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.4197 - acc: 0.8312 - val_loss: 0.4256 - val_acc: 0.8268\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 149us/step - loss: 0.4312 - acc: 0.8168 - val_loss: 0.4402 - val_acc: 0.8346\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4322 - acc: 0.8194 - val_loss: 0.4330 - val_acc: 0.8189\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 140us/step - loss: 0.4345 - acc: 0.8285 - val_loss: 0.4263 - val_acc: 0.8189\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4350 - acc: 0.8259 - val_loss: 0.4236 - val_acc: 0.8268\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4275 - acc: 0.8325 - val_loss: 0.4272 - val_acc: 0.8346\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 157us/step - loss: 0.4218 - acc: 0.8272 - val_loss: 0.4307 - val_acc: 0.8425\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 151us/step - loss: 0.4243 - acc: 0.8312 - val_loss: 0.4230 - val_acc: 0.8189\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 146us/step - loss: 0.4255 - acc: 0.8285 - val_loss: 0.4255 - val_acc: 0.8268\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4183 - acc: 0.8429 - val_loss: 0.4211 - val_acc: 0.8189\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.4132 - acc: 0.8364 - val_loss: 0.4296 - val_acc: 0.8189\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4067 - acc: 0.8364 - val_loss: 0.4259 - val_acc: 0.8346\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4273 - acc: 0.8298 - val_loss: 0.4198 - val_acc: 0.8189\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4121 - acc: 0.8390 - val_loss: 0.4315 - val_acc: 0.8425\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 147us/step - loss: 0.4248 - acc: 0.8259 - val_loss: 0.4202 - val_acc: 0.8425\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 177us/step - loss: 0.4026 - acc: 0.8298 - val_loss: 0.4143 - val_acc: 0.8346\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 165us/step - loss: 0.3982 - acc: 0.8455 - val_loss: 0.4209 - val_acc: 0.8504\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 150us/step - loss: 0.4095 - acc: 0.8403 - val_loss: 0.4231 - val_acc: 0.8189\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.4029 - acc: 0.8364 - val_loss: 0.4220 - val_acc: 0.8425\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4048 - acc: 0.8455 - val_loss: 0.4288 - val_acc: 0.8346\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4322 - acc: 0.8220 - val_loss: 0.4301 - val_acc: 0.8425\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4234 - acc: 0.8338 - val_loss: 0.4160 - val_acc: 0.8425\n",
      "processing fold # 1\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 519us/step - loss: 0.6459 - acc: 0.6270 - val_loss: 0.5910 - val_acc: 0.6929\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 149us/step - loss: 0.5341 - acc: 0.7631 - val_loss: 0.5334 - val_acc: 0.7559\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 165us/step - loss: 0.5078 - acc: 0.7801 - val_loss: 0.5219 - val_acc: 0.7638\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4946 - acc: 0.7788 - val_loss: 0.5166 - val_acc: 0.7638\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 194us/step - loss: 0.4866 - acc: 0.7997 - val_loss: 0.5169 - val_acc: 0.7402\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 202us/step - loss: 0.4676 - acc: 0.7932 - val_loss: 0.5176 - val_acc: 0.7402\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 171us/step - loss: 0.4617 - acc: 0.8154 - val_loss: 0.5125 - val_acc: 0.7638\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4719 - acc: 0.8063 - val_loss: 0.5136 - val_acc: 0.7480\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.4673 - acc: 0.7997 - val_loss: 0.5159 - val_acc: 0.7402\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 168us/step - loss: 0.4438 - acc: 0.8141 - val_loss: 0.5209 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4505 - acc: 0.8115 - val_loss: 0.5116 - val_acc: 0.7638\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4518 - acc: 0.8050 - val_loss: 0.5084 - val_acc: 0.7638\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 151us/step - loss: 0.4441 - acc: 0.8128 - val_loss: 0.5100 - val_acc: 0.7638\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 180us/step - loss: 0.4352 - acc: 0.8181 - val_loss: 0.5144 - val_acc: 0.7795\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 194us/step - loss: 0.4494 - acc: 0.8076 - val_loss: 0.5090 - val_acc: 0.7795\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4311 - acc: 0.8194 - val_loss: 0.5126 - val_acc: 0.7559\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 151us/step - loss: 0.4192 - acc: 0.8233 - val_loss: 0.5171 - val_acc: 0.7874\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 197us/step - loss: 0.4119 - acc: 0.8298 - val_loss: 0.5285 - val_acc: 0.7795\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4375 - acc: 0.8220 - val_loss: 0.5262 - val_acc: 0.7795\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4404 - acc: 0.8246 - val_loss: 0.5219 - val_acc: 0.7874\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4095 - acc: 0.8285 - val_loss: 0.5299 - val_acc: 0.7795\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 181us/step - loss: 0.4197 - acc: 0.8325 - val_loss: 0.5374 - val_acc: 0.7717\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4177 - acc: 0.8233 - val_loss: 0.5259 - val_acc: 0.7795\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4310 - acc: 0.8220 - val_loss: 0.5299 - val_acc: 0.7795\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4061 - acc: 0.8246 - val_loss: 0.5361 - val_acc: 0.7795\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 107us/step - loss: 0.4106 - acc: 0.8325 - val_loss: 0.5364 - val_acc: 0.7717\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.4022 - acc: 0.8364 - val_loss: 0.5459 - val_acc: 0.7795\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4234 - acc: 0.8233 - val_loss: 0.5381 - val_acc: 0.7795\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4112 - acc: 0.8325 - val_loss: 0.5549 - val_acc: 0.7795\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4049 - acc: 0.8338 - val_loss: 0.5509 - val_acc: 0.7874\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4069 - acc: 0.8416 - val_loss: 0.5520 - val_acc: 0.7874\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4199 - acc: 0.8259 - val_loss: 0.5522 - val_acc: 0.7795\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4050 - acc: 0.8246 - val_loss: 0.5644 - val_acc: 0.7874\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.3999 - acc: 0.8403 - val_loss: 0.5613 - val_acc: 0.7953\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.3923 - acc: 0.8364 - val_loss: 0.5692 - val_acc: 0.7953\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 171us/step - loss: 0.3996 - acc: 0.8312 - val_loss: 0.5809 - val_acc: 0.7953\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 187us/step - loss: 0.3822 - acc: 0.8455 - val_loss: 0.5884 - val_acc: 0.7953\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 225us/step - loss: 0.3854 - acc: 0.8442 - val_loss: 0.5916 - val_acc: 0.7953\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 244us/step - loss: 0.3909 - acc: 0.8455 - val_loss: 0.5891 - val_acc: 0.8031\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 168us/step - loss: 0.3832 - acc: 0.8469 - val_loss: 0.6048 - val_acc: 0.8031\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 207us/step - loss: 0.3841 - acc: 0.8377 - val_loss: 0.6110 - val_acc: 0.8031\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 222us/step - loss: 0.3908 - acc: 0.8390 - val_loss: 0.5994 - val_acc: 0.7953\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 254us/step - loss: 0.3960 - acc: 0.8469 - val_loss: 0.6055 - val_acc: 0.7953\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 182us/step - loss: 0.3831 - acc: 0.8429 - val_loss: 0.6206 - val_acc: 0.7953\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 175us/step - loss: 0.3889 - acc: 0.8429 - val_loss: 0.6185 - val_acc: 0.8110\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 178us/step - loss: 0.3950 - acc: 0.8416 - val_loss: 0.6078 - val_acc: 0.8031\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 171us/step - loss: 0.3838 - acc: 0.8429 - val_loss: 0.6086 - val_acc: 0.7953\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 167us/step - loss: 0.3881 - acc: 0.8364 - val_loss: 0.6333 - val_acc: 0.7953\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 195us/step - loss: 0.3779 - acc: 0.8455 - val_loss: 0.6382 - val_acc: 0.7953\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 152us/step - loss: 0.3972 - acc: 0.8377 - val_loss: 0.6264 - val_acc: 0.8110\n",
      "processing fold # 2\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 1s 726us/step - loss: 0.6435 - acc: 0.6322 - val_loss: 0.5883 - val_acc: 0.7953\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 174us/step - loss: 0.5460 - acc: 0.7644 - val_loss: 0.5051 - val_acc: 0.7874\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.5314 - acc: 0.7749 - val_loss: 0.4758 - val_acc: 0.7874\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4914 - acc: 0.7788 - val_loss: 0.4645 - val_acc: 0.7953\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 244us/step - loss: 0.4884 - acc: 0.7893 - val_loss: 0.4556 - val_acc: 0.7953\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4842 - acc: 0.7749 - val_loss: 0.4482 - val_acc: 0.7953\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4758 - acc: 0.7919 - val_loss: 0.4447 - val_acc: 0.8031\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 157us/step - loss: 0.4659 - acc: 0.7840 - val_loss: 0.4409 - val_acc: 0.8031\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 150us/step - loss: 0.4651 - acc: 0.8076 - val_loss: 0.4336 - val_acc: 0.8031\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4708 - acc: 0.7997 - val_loss: 0.4323 - val_acc: 0.8031\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4547 - acc: 0.8037 - val_loss: 0.4293 - val_acc: 0.8110\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4362 - acc: 0.8076 - val_loss: 0.4200 - val_acc: 0.8110\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4499 - acc: 0.7984 - val_loss: 0.4206 - val_acc: 0.8031\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4422 - acc: 0.8063 - val_loss: 0.4200 - val_acc: 0.8189\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4521 - acc: 0.8115 - val_loss: 0.4110 - val_acc: 0.8110\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4491 - acc: 0.8010 - val_loss: 0.4065 - val_acc: 0.8110\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4532 - acc: 0.8037 - val_loss: 0.4066 - val_acc: 0.8110\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4490 - acc: 0.8037 - val_loss: 0.4051 - val_acc: 0.8189\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4506 - acc: 0.8050 - val_loss: 0.4015 - val_acc: 0.8346\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4373 - acc: 0.8285 - val_loss: 0.3993 - val_acc: 0.8346\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.4317 - acc: 0.8233 - val_loss: 0.3964 - val_acc: 0.8346\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4397 - acc: 0.8233 - val_loss: 0.3960 - val_acc: 0.8189\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 181us/step - loss: 0.4361 - acc: 0.8168 - val_loss: 0.3940 - val_acc: 0.8189\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 162us/step - loss: 0.4477 - acc: 0.8207 - val_loss: 0.3942 - val_acc: 0.8268\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4346 - acc: 0.8246 - val_loss: 0.3895 - val_acc: 0.8346\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 163us/step - loss: 0.4266 - acc: 0.8272 - val_loss: 0.3923 - val_acc: 0.8346\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4277 - acc: 0.8325 - val_loss: 0.3885 - val_acc: 0.8268\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 185us/step - loss: 0.4295 - acc: 0.8351 - val_loss: 0.3834 - val_acc: 0.8189\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 155us/step - loss: 0.4303 - acc: 0.8285 - val_loss: 0.3849 - val_acc: 0.8268\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4202 - acc: 0.8181 - val_loss: 0.3835 - val_acc: 0.8268\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4083 - acc: 0.8364 - val_loss: 0.3833 - val_acc: 0.8425\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4215 - acc: 0.8338 - val_loss: 0.3848 - val_acc: 0.8425\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4053 - acc: 0.8351 - val_loss: 0.3800 - val_acc: 0.8425\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4222 - acc: 0.8377 - val_loss: 0.3820 - val_acc: 0.8425\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4152 - acc: 0.8338 - val_loss: 0.3807 - val_acc: 0.8425\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4265 - acc: 0.8272 - val_loss: 0.3812 - val_acc: 0.8425\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4191 - acc: 0.8377 - val_loss: 0.3771 - val_acc: 0.8268\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4054 - acc: 0.8429 - val_loss: 0.3755 - val_acc: 0.8268\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4222 - acc: 0.8220 - val_loss: 0.3811 - val_acc: 0.8346\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4133 - acc: 0.8403 - val_loss: 0.3762 - val_acc: 0.8425\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4222 - acc: 0.8377 - val_loss: 0.3789 - val_acc: 0.8504\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 179us/step - loss: 0.4230 - acc: 0.8364 - val_loss: 0.3830 - val_acc: 0.8425\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 183us/step - loss: 0.4263 - acc: 0.8338 - val_loss: 0.3825 - val_acc: 0.8425\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 258us/step - loss: 0.4212 - acc: 0.8377 - val_loss: 0.3795 - val_acc: 0.8425\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 187us/step - loss: 0.4022 - acc: 0.8390 - val_loss: 0.3779 - val_acc: 0.8425\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 181us/step - loss: 0.4255 - acc: 0.8351 - val_loss: 0.3759 - val_acc: 0.8504\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 223us/step - loss: 0.4141 - acc: 0.8429 - val_loss: 0.3752 - val_acc: 0.8425\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 263us/step - loss: 0.4111 - acc: 0.8416 - val_loss: 0.3748 - val_acc: 0.8425\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 186us/step - loss: 0.4171 - acc: 0.8416 - val_loss: 0.3779 - val_acc: 0.8268\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 174us/step - loss: 0.4106 - acc: 0.8442 - val_loss: 0.3763 - val_acc: 0.8504\n",
      "processing fold # 3\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 640us/step - loss: 0.6102 - acc: 0.6453 - val_loss: 0.5836 - val_acc: 0.7402\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.5471 - acc: 0.7356 - val_loss: 0.5541 - val_acc: 0.7480\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 211us/step - loss: 0.5200 - acc: 0.7762 - val_loss: 0.5500 - val_acc: 0.7480\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 167us/step - loss: 0.5154 - acc: 0.7840 - val_loss: 0.5485 - val_acc: 0.7480\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4972 - acc: 0.7958 - val_loss: 0.5330 - val_acc: 0.7559\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4666 - acc: 0.7971 - val_loss: 0.5342 - val_acc: 0.7559\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4762 - acc: 0.7997 - val_loss: 0.5182 - val_acc: 0.7559\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4577 - acc: 0.8207 - val_loss: 0.5095 - val_acc: 0.7559\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4896 - acc: 0.7958 - val_loss: 0.4946 - val_acc: 0.7638\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4634 - acc: 0.8102 - val_loss: 0.4899 - val_acc: 0.7638\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4580 - acc: 0.8154 - val_loss: 0.4916 - val_acc: 0.7874\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4541 - acc: 0.8089 - val_loss: 0.4778 - val_acc: 0.7874\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 223us/step - loss: 0.4564 - acc: 0.8102 - val_loss: 0.4714 - val_acc: 0.7874\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4398 - acc: 0.8115 - val_loss: 0.4724 - val_acc: 0.7874\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4627 - acc: 0.8102 - val_loss: 0.4668 - val_acc: 0.8031\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4313 - acc: 0.8141 - val_loss: 0.4721 - val_acc: 0.8031\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 224us/step - loss: 0.4359 - acc: 0.8115 - val_loss: 0.4613 - val_acc: 0.7874\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4456 - acc: 0.8181 - val_loss: 0.4653 - val_acc: 0.7953\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 184us/step - loss: 0.4520 - acc: 0.8168 - val_loss: 0.4685 - val_acc: 0.7874\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.4353 - acc: 0.8168 - val_loss: 0.4621 - val_acc: 0.7953\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4432 - acc: 0.8154 - val_loss: 0.4579 - val_acc: 0.7874\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 183us/step - loss: 0.4284 - acc: 0.8102 - val_loss: 0.4601 - val_acc: 0.7795\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4318 - acc: 0.8181 - val_loss: 0.4536 - val_acc: 0.7874\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4228 - acc: 0.8246 - val_loss: 0.4521 - val_acc: 0.7874\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4340 - acc: 0.8168 - val_loss: 0.4581 - val_acc: 0.7795\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4144 - acc: 0.8377 - val_loss: 0.4661 - val_acc: 0.7874\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 230us/step - loss: 0.4309 - acc: 0.8181 - val_loss: 0.4613 - val_acc: 0.7953\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 180us/step - loss: 0.4300 - acc: 0.8272 - val_loss: 0.4517 - val_acc: 0.8031\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4289 - acc: 0.8233 - val_loss: 0.4584 - val_acc: 0.7953\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 108us/step - loss: 0.4443 - acc: 0.8141 - val_loss: 0.4555 - val_acc: 0.7953\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 265us/step - loss: 0.4253 - acc: 0.8298 - val_loss: 0.4536 - val_acc: 0.7953\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4226 - acc: 0.8207 - val_loss: 0.4568 - val_acc: 0.8031\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4128 - acc: 0.8338 - val_loss: 0.4615 - val_acc: 0.8031\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 215us/step - loss: 0.4202 - acc: 0.8246 - val_loss: 0.4628 - val_acc: 0.7953\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 160us/step - loss: 0.4303 - acc: 0.8272 - val_loss: 0.4600 - val_acc: 0.7953\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4291 - acc: 0.8272 - val_loss: 0.4571 - val_acc: 0.7953\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4291 - acc: 0.8298 - val_loss: 0.4585 - val_acc: 0.7953\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4275 - acc: 0.8338 - val_loss: 0.4629 - val_acc: 0.7953\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 281us/step - loss: 0.4161 - acc: 0.8455 - val_loss: 0.4688 - val_acc: 0.7953\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4239 - acc: 0.8403 - val_loss: 0.4717 - val_acc: 0.7874\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4197 - acc: 0.8338 - val_loss: 0.4626 - val_acc: 0.7874\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4227 - acc: 0.8351 - val_loss: 0.4673 - val_acc: 0.7874\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4177 - acc: 0.8390 - val_loss: 0.4612 - val_acc: 0.7874\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4219 - acc: 0.8285 - val_loss: 0.4650 - val_acc: 0.7953\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4207 - acc: 0.8298 - val_loss: 0.4654 - val_acc: 0.7874\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 226us/step - loss: 0.4177 - acc: 0.8325 - val_loss: 0.4647 - val_acc: 0.7874\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 151us/step - loss: 0.4265 - acc: 0.8325 - val_loss: 0.4591 - val_acc: 0.7874\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 128us/step - loss: 0.4142 - acc: 0.8272 - val_loss: 0.4644 - val_acc: 0.7953\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4165 - acc: 0.8351 - val_loss: 0.4603 - val_acc: 0.8031\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 158us/step - loss: 0.4095 - acc: 0.8312 - val_loss: 0.4607 - val_acc: 0.7953\n",
      "processing fold # 4\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 598us/step - loss: 0.6259 - acc: 0.6780 - val_loss: 0.5447 - val_acc: 0.7795\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.5621 - acc: 0.7369 - val_loss: 0.4878 - val_acc: 0.7795\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.5349 - acc: 0.7762 - val_loss: 0.4646 - val_acc: 0.7795\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.5110 - acc: 0.7919 - val_loss: 0.4572 - val_acc: 0.7795\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 237us/step - loss: 0.5159 - acc: 0.7827 - val_loss: 0.4474 - val_acc: 0.8110\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.5011 - acc: 0.7853 - val_loss: 0.4435 - val_acc: 0.8031\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 158us/step - loss: 0.4972 - acc: 0.7866 - val_loss: 0.4387 - val_acc: 0.8031\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 197us/step - loss: 0.4913 - acc: 0.7919 - val_loss: 0.4391 - val_acc: 0.7953\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.4872 - acc: 0.7801 - val_loss: 0.4384 - val_acc: 0.7953\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 183us/step - loss: 0.4599 - acc: 0.8194 - val_loss: 0.4366 - val_acc: 0.7953\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4588 - acc: 0.8010 - val_loss: 0.4353 - val_acc: 0.8031\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4634 - acc: 0.7997 - val_loss: 0.4303 - val_acc: 0.8031\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4607 - acc: 0.8141 - val_loss: 0.4295 - val_acc: 0.8268\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4510 - acc: 0.8102 - val_loss: 0.4188 - val_acc: 0.8189\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 218us/step - loss: 0.4614 - acc: 0.8207 - val_loss: 0.4185 - val_acc: 0.8189\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 174us/step - loss: 0.4572 - acc: 0.8194 - val_loss: 0.4198 - val_acc: 0.8189\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 148us/step - loss: 0.4546 - acc: 0.8168 - val_loss: 0.4159 - val_acc: 0.8268\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4505 - acc: 0.8181 - val_loss: 0.4206 - val_acc: 0.7874\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4419 - acc: 0.8246 - val_loss: 0.4191 - val_acc: 0.8189\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4402 - acc: 0.8220 - val_loss: 0.4130 - val_acc: 0.8346\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 169us/step - loss: 0.4386 - acc: 0.8220 - val_loss: 0.4146 - val_acc: 0.8110\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4537 - acc: 0.8233 - val_loss: 0.4168 - val_acc: 0.8110\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4357 - acc: 0.8351 - val_loss: 0.4176 - val_acc: 0.8110\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4389 - acc: 0.8220 - val_loss: 0.4201 - val_acc: 0.8110\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4383 - acc: 0.8351 - val_loss: 0.4191 - val_acc: 0.8031\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4156 - acc: 0.8285 - val_loss: 0.4249 - val_acc: 0.7953\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.4336 - acc: 0.8377 - val_loss: 0.4270 - val_acc: 0.8031\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 204us/step - loss: 0.4194 - acc: 0.8285 - val_loss: 0.4258 - val_acc: 0.8189\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4470 - acc: 0.8246 - val_loss: 0.4299 - val_acc: 0.8031\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4275 - acc: 0.8285 - val_loss: 0.4278 - val_acc: 0.8031\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4257 - acc: 0.8259 - val_loss: 0.4275 - val_acc: 0.8110\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 227us/step - loss: 0.4287 - acc: 0.8364 - val_loss: 0.4258 - val_acc: 0.8189\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4274 - acc: 0.8351 - val_loss: 0.4283 - val_acc: 0.8031\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4382 - acc: 0.8285 - val_loss: 0.4295 - val_acc: 0.8031\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.4142 - acc: 0.8351 - val_loss: 0.4307 - val_acc: 0.8031\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4211 - acc: 0.8312 - val_loss: 0.4318 - val_acc: 0.8031\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4179 - acc: 0.8285 - val_loss: 0.4382 - val_acc: 0.8031\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4327 - acc: 0.8312 - val_loss: 0.4384 - val_acc: 0.8110\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4298 - acc: 0.8429 - val_loss: 0.4345 - val_acc: 0.8189\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 165us/step - loss: 0.4122 - acc: 0.8351 - val_loss: 0.4414 - val_acc: 0.8031\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4118 - acc: 0.8429 - val_loss: 0.4373 - val_acc: 0.8031\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 233us/step - loss: 0.4250 - acc: 0.8325 - val_loss: 0.4383 - val_acc: 0.8110\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4291 - acc: 0.8338 - val_loss: 0.4359 - val_acc: 0.8110\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4119 - acc: 0.8416 - val_loss: 0.4382 - val_acc: 0.8031\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4099 - acc: 0.8364 - val_loss: 0.4346 - val_acc: 0.8110\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4165 - acc: 0.8325 - val_loss: 0.4388 - val_acc: 0.8031\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 165us/step - loss: 0.4184 - acc: 0.8351 - val_loss: 0.4414 - val_acc: 0.8031\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 138us/step - loss: 0.4163 - acc: 0.8416 - val_loss: 0.4454 - val_acc: 0.8031\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 126us/step - loss: 0.4091 - acc: 0.8364 - val_loss: 0.4456 - val_acc: 0.8031\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4163 - acc: 0.8312 - val_loss: 0.4474 - val_acc: 0.8031\n",
      "processing fold # 5\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n",
      "764/764 [==============================] - 0s 607us/step - loss: 0.6190 - acc: 0.6623 - val_loss: 0.5545 - val_acc: 0.7402\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.5664 - acc: 0.7408 - val_loss: 0.5078 - val_acc: 0.7874\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 127us/step - loss: 0.5335 - acc: 0.7736 - val_loss: 0.4877 - val_acc: 0.7795\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.5111 - acc: 0.7723 - val_loss: 0.4756 - val_acc: 0.7795\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 134us/step - loss: 0.4929 - acc: 0.7827 - val_loss: 0.4650 - val_acc: 0.7874\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4984 - acc: 0.7906 - val_loss: 0.4581 - val_acc: 0.7953\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4811 - acc: 0.7853 - val_loss: 0.4492 - val_acc: 0.8031\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4776 - acc: 0.7932 - val_loss: 0.4436 - val_acc: 0.8031\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4786 - acc: 0.7919 - val_loss: 0.4407 - val_acc: 0.8031\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4856 - acc: 0.7945 - val_loss: 0.4335 - val_acc: 0.8110\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 142us/step - loss: 0.4591 - acc: 0.7984 - val_loss: 0.4266 - val_acc: 0.8346\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4738 - acc: 0.7945 - val_loss: 0.4295 - val_acc: 0.8189\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4562 - acc: 0.8024 - val_loss: 0.4202 - val_acc: 0.8268\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 135us/step - loss: 0.4562 - acc: 0.7932 - val_loss: 0.4162 - val_acc: 0.8268\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 143us/step - loss: 0.4561 - acc: 0.8128 - val_loss: 0.4170 - val_acc: 0.8189\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4503 - acc: 0.8076 - val_loss: 0.4119 - val_acc: 0.8268\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4468 - acc: 0.8128 - val_loss: 0.4113 - val_acc: 0.8268\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4242 - acc: 0.8259 - val_loss: 0.4056 - val_acc: 0.8346\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.4376 - acc: 0.8037 - val_loss: 0.4023 - val_acc: 0.8425\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4406 - acc: 0.8168 - val_loss: 0.4081 - val_acc: 0.8346\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 161us/step - loss: 0.4389 - acc: 0.8220 - val_loss: 0.4028 - val_acc: 0.8346\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4495 - acc: 0.8115 - val_loss: 0.4065 - val_acc: 0.8346\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 132us/step - loss: 0.4129 - acc: 0.8194 - val_loss: 0.3951 - val_acc: 0.8583\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 193us/step - loss: 0.4336 - acc: 0.8194 - val_loss: 0.3949 - val_acc: 0.8504\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 254us/step - loss: 0.4433 - acc: 0.8154 - val_loss: 0.3999 - val_acc: 0.8346\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 152us/step - loss: 0.4275 - acc: 0.8259 - val_loss: 0.3942 - val_acc: 0.8583\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 164us/step - loss: 0.4349 - acc: 0.8207 - val_loss: 0.3975 - val_acc: 0.8504\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4344 - acc: 0.8312 - val_loss: 0.3954 - val_acc: 0.8425\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4356 - acc: 0.8220 - val_loss: 0.4073 - val_acc: 0.8189\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4399 - acc: 0.8194 - val_loss: 0.3994 - val_acc: 0.8268\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 173us/step - loss: 0.4280 - acc: 0.8246 - val_loss: 0.3958 - val_acc: 0.8346\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 204us/step - loss: 0.4310 - acc: 0.8194 - val_loss: 0.3944 - val_acc: 0.8425\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 192us/step - loss: 0.4139 - acc: 0.8220 - val_loss: 0.3936 - val_acc: 0.8425\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 136us/step - loss: 0.4230 - acc: 0.8338 - val_loss: 0.3967 - val_acc: 0.8268\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 144us/step - loss: 0.4245 - acc: 0.8298 - val_loss: 0.3933 - val_acc: 0.8425\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 171us/step - loss: 0.4173 - acc: 0.8285 - val_loss: 0.3954 - val_acc: 0.8346\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4080 - acc: 0.8416 - val_loss: 0.3910 - val_acc: 0.8583\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4362 - acc: 0.8272 - val_loss: 0.3993 - val_acc: 0.8110\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 152us/step - loss: 0.4149 - acc: 0.8285 - val_loss: 0.3880 - val_acc: 0.8661\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 244us/step - loss: 0.4012 - acc: 0.8403 - val_loss: 0.3885 - val_acc: 0.8504\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 157us/step - loss: 0.4263 - acc: 0.8246 - val_loss: 0.3838 - val_acc: 0.8661\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4263 - acc: 0.8181 - val_loss: 0.3899 - val_acc: 0.8504\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4169 - acc: 0.8298 - val_loss: 0.3858 - val_acc: 0.8583\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4093 - acc: 0.8377 - val_loss: 0.3841 - val_acc: 0.8583\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4177 - acc: 0.8312 - val_loss: 0.3929 - val_acc: 0.8110\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4155 - acc: 0.8312 - val_loss: 0.3897 - val_acc: 0.8189\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4026 - acc: 0.8429 - val_loss: 0.3887 - val_acc: 0.8268\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 112us/step - loss: 0.4301 - acc: 0.8194 - val_loss: 0.3870 - val_acc: 0.8268\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4064 - acc: 0.8403 - val_loss: 0.3849 - val_acc: 0.8346\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 130us/step - loss: 0.4066 - acc: 0.8312 - val_loss: 0.3900 - val_acc: 0.8189\n",
      "processing fold # 6\n",
      "Train on 764 samples, validate on 127 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/764 [==============================] - 0s 590us/step - loss: 0.6299 - acc: 0.6584 - val_loss: 0.5228 - val_acc: 0.8504\n",
      "Epoch 2/50\n",
      "764/764 [==============================] - 0s 107us/step - loss: 0.5727 - acc: 0.7408 - val_loss: 0.4599 - val_acc: 0.8268\n",
      "Epoch 3/50\n",
      "764/764 [==============================] - 0s 133us/step - loss: 0.5280 - acc: 0.7814 - val_loss: 0.4408 - val_acc: 0.8268\n",
      "Epoch 4/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.5201 - acc: 0.7696 - val_loss: 0.4189 - val_acc: 0.8425\n",
      "Epoch 5/50\n",
      "764/764 [==============================] - 0s 113us/step - loss: 0.5038 - acc: 0.7853 - val_loss: 0.4135 - val_acc: 0.8346\n",
      "Epoch 6/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.5076 - acc: 0.7801 - val_loss: 0.4090 - val_acc: 0.8504\n",
      "Epoch 7/50\n",
      "764/764 [==============================] - 0s 109us/step - loss: 0.4946 - acc: 0.7762 - val_loss: 0.4061 - val_acc: 0.8504\n",
      "Epoch 8/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4944 - acc: 0.7971 - val_loss: 0.4059 - val_acc: 0.8504\n",
      "Epoch 9/50\n",
      "764/764 [==============================] - 0s 122us/step - loss: 0.4792 - acc: 0.7945 - val_loss: 0.4051 - val_acc: 0.8504\n",
      "Epoch 10/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4870 - acc: 0.7893 - val_loss: 0.3947 - val_acc: 0.8583\n",
      "Epoch 11/50\n",
      "764/764 [==============================] - 0s 106us/step - loss: 0.4613 - acc: 0.8050 - val_loss: 0.4063 - val_acc: 0.8425\n",
      "Epoch 12/50\n",
      "764/764 [==============================] - 0s 109us/step - loss: 0.4522 - acc: 0.8128 - val_loss: 0.3828 - val_acc: 0.8583\n",
      "Epoch 13/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4696 - acc: 0.7984 - val_loss: 0.3851 - val_acc: 0.8583\n",
      "Epoch 14/50\n",
      "764/764 [==============================] - 0s 194us/step - loss: 0.4542 - acc: 0.7984 - val_loss: 0.3831 - val_acc: 0.8583\n",
      "Epoch 15/50\n",
      "764/764 [==============================] - 0s 139us/step - loss: 0.4767 - acc: 0.7984 - val_loss: 0.3935 - val_acc: 0.8583\n",
      "Epoch 16/50\n",
      "764/764 [==============================] - 0s 242us/step - loss: 0.4451 - acc: 0.8063 - val_loss: 0.3853 - val_acc: 0.8583\n",
      "Epoch 17/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4538 - acc: 0.8063 - val_loss: 0.3906 - val_acc: 0.8583\n",
      "Epoch 18/50\n",
      "764/764 [==============================] - 0s 111us/step - loss: 0.4369 - acc: 0.8141 - val_loss: 0.3899 - val_acc: 0.8583\n",
      "Epoch 19/50\n",
      "764/764 [==============================] - 0s 123us/step - loss: 0.4488 - acc: 0.8233 - val_loss: 0.3879 - val_acc: 0.8583\n",
      "Epoch 20/50\n",
      "764/764 [==============================] - 0s 108us/step - loss: 0.4564 - acc: 0.7997 - val_loss: 0.3886 - val_acc: 0.8504\n",
      "Epoch 21/50\n",
      "764/764 [==============================] - 0s 131us/step - loss: 0.4471 - acc: 0.7984 - val_loss: 0.3810 - val_acc: 0.8583\n",
      "Epoch 22/50\n",
      "764/764 [==============================] - 0s 120us/step - loss: 0.4553 - acc: 0.8128 - val_loss: 0.3806 - val_acc: 0.8583\n",
      "Epoch 23/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4452 - acc: 0.8233 - val_loss: 0.3840 - val_acc: 0.8583\n",
      "Epoch 24/50\n",
      "764/764 [==============================] - 0s 110us/step - loss: 0.4350 - acc: 0.8259 - val_loss: 0.3846 - val_acc: 0.8583\n",
      "Epoch 25/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4484 - acc: 0.8115 - val_loss: 0.3891 - val_acc: 0.8661\n",
      "Epoch 26/50\n",
      "764/764 [==============================] - 0s 223us/step - loss: 0.4512 - acc: 0.8115 - val_loss: 0.3867 - val_acc: 0.8661\n",
      "Epoch 27/50\n",
      "764/764 [==============================] - 0s 137us/step - loss: 0.4205 - acc: 0.8259 - val_loss: 0.3852 - val_acc: 0.8661\n",
      "Epoch 28/50\n",
      "764/764 [==============================] - 0s 145us/step - loss: 0.4238 - acc: 0.8102 - val_loss: 0.3883 - val_acc: 0.8661\n",
      "Epoch 29/50\n",
      "764/764 [==============================] - 0s 125us/step - loss: 0.4315 - acc: 0.8128 - val_loss: 0.3886 - val_acc: 0.8661\n",
      "Epoch 30/50\n",
      "764/764 [==============================] - 0s 121us/step - loss: 0.4322 - acc: 0.8207 - val_loss: 0.3902 - val_acc: 0.8583\n",
      "Epoch 31/50\n",
      "764/764 [==============================] - 0s 116us/step - loss: 0.4443 - acc: 0.8168 - val_loss: 0.3979 - val_acc: 0.8583\n",
      "Epoch 32/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4233 - acc: 0.8194 - val_loss: 0.3891 - val_acc: 0.8661\n",
      "Epoch 33/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4231 - acc: 0.8272 - val_loss: 0.3963 - val_acc: 0.8583\n",
      "Epoch 34/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4368 - acc: 0.8207 - val_loss: 0.3945 - val_acc: 0.8583\n",
      "Epoch 35/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4169 - acc: 0.8246 - val_loss: 0.3898 - val_acc: 0.8661\n",
      "Epoch 36/50\n",
      "764/764 [==============================] - 0s 129us/step - loss: 0.4213 - acc: 0.8259 - val_loss: 0.3955 - val_acc: 0.8583\n",
      "Epoch 37/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4157 - acc: 0.8325 - val_loss: 0.3964 - val_acc: 0.8504\n",
      "Epoch 38/50\n",
      "764/764 [==============================] - 0s 115us/step - loss: 0.4348 - acc: 0.8259 - val_loss: 0.3881 - val_acc: 0.8661\n",
      "Epoch 39/50\n",
      "764/764 [==============================] - 0s 194us/step - loss: 0.4171 - acc: 0.8246 - val_loss: 0.3917 - val_acc: 0.8661\n",
      "Epoch 40/50\n",
      "764/764 [==============================] - 0s 169us/step - loss: 0.4288 - acc: 0.8325 - val_loss: 0.3964 - val_acc: 0.8661\n",
      "Epoch 41/50\n",
      "764/764 [==============================] - 0s 141us/step - loss: 0.4154 - acc: 0.8207 - val_loss: 0.3942 - val_acc: 0.8661\n",
      "Epoch 42/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4226 - acc: 0.8259 - val_loss: 0.3936 - val_acc: 0.8425\n",
      "Epoch 43/50\n",
      "764/764 [==============================] - 0s 114us/step - loss: 0.4181 - acc: 0.8338 - val_loss: 0.3953 - val_acc: 0.8583\n",
      "Epoch 44/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4185 - acc: 0.8442 - val_loss: 0.3990 - val_acc: 0.8583\n",
      "Epoch 45/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4188 - acc: 0.8259 - val_loss: 0.3917 - val_acc: 0.8504\n",
      "Epoch 46/50\n",
      "764/764 [==============================] - 0s 124us/step - loss: 0.4203 - acc: 0.8272 - val_loss: 0.3933 - val_acc: 0.8583\n",
      "Epoch 47/50\n",
      "764/764 [==============================] - 0s 118us/step - loss: 0.4327 - acc: 0.8207 - val_loss: 0.3934 - val_acc: 0.8583\n",
      "Epoch 48/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4060 - acc: 0.8377 - val_loss: 0.3940 - val_acc: 0.8661\n",
      "Epoch 49/50\n",
      "764/764 [==============================] - 0s 119us/step - loss: 0.4168 - acc: 0.8285 - val_loss: 0.3922 - val_acc: 0.8583\n",
      "Epoch 50/50\n",
      "764/764 [==============================] - 0s 117us/step - loss: 0.4110 - acc: 0.8338 - val_loss: 0.3949 - val_acc: 0.8583\n"
     ]
    }
   ],
   "source": [
    "x_train = normalized_train_data.drop([\"Survived\"], axis=1).values\n",
    "y_train = normalized_train_data[\"Survived\"].values\n",
    "\n",
    "number_of_epochs = 50\n",
    "\n",
    "number_of_folds = 7\n",
    "number_of_samples = len(x_train) // number_of_folds\n",
    "\n",
    "all_histories = []\n",
    "for i in range(number_of_folds):\n",
    "    print(\"processing fold #\", i)\n",
    "    \n",
    "    partial_x_train = np.concatenate([x_train[:i*number_of_samples],\n",
    "                                          x_train[(i+1)*number_of_samples:]])\n",
    "    parital_y_train = np.concatenate([y_train[:i*number_of_samples],\n",
    "                                          y_train[(i+1)*number_of_samples:]])\n",
    "    \n",
    "    partial_x_validation = x_train[i*number_of_samples:(i+1)*number_of_samples]\n",
    "    partial_y_validation = y_train[i*number_of_samples:(i+1)*number_of_samples]\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation=\"relu\", input_shape=(x_train.shape[1],)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(RMSprop(lr=0.001),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"acc\"])\n",
    "\n",
    "    history = model.fit(partial_x_train,\n",
    "                        parital_y_train,\n",
    "                        epochs=number_of_epochs,\n",
    "                        batch_size=16,\n",
    "                        validation_data=[partial_x_validation,partial_y_validation])\n",
    "    all_histories.append(history.history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "loss_values = [np.mean([x[\"loss\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "val_loss_values = [np.mean([x[\"val_loss\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\", color=\"red\")\n",
    "plt.axhline(y=0.44)\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "acc_values = [np.mean([x[\"acc\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "val_acc_values = [np.mean([x[\"val_acc\"][i] for x in all_histories]) for i in range(number_of_epochs)]\n",
    "\n",
    "plt.plot(epochs, acc_values, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc_values, \"b\", label=\"Validation acc\", color=\"red\")\n",
    "plt.axhline(y=0.82)\n",
    "plt.title(\"Training and validation acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived\n",
       "PassengerId          \n",
       "892                 0\n",
       "893                 0\n",
       "894                 0\n",
       "895                 0\n",
       "896                 1\n",
       "...               ...\n",
       "1300                1\n",
       "1302                1\n",
       "1305                0\n",
       "1308                0\n",
       "1309                1\n",
       "\n",
       "[418 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.predict(normalized_test_data.drop([\"Survived\"], axis=1))\n",
    "results = pd.DataFrame(results, columns=[\"Survived\"], index=normalized_test_data.index)\n",
    "\n",
    "results.loc[results.Survived < 0.5, [\"Survived\"]] = 0\n",
    "results.loc[results.Survived >= 0.5, [\"Survived\"]] = 1\n",
    "results = results.fillna(0)\n",
    "results.Survived = results.Survived.astype(int)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"../Output/my_prediction.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with genderr submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Example</th>\n",
       "      <th>MyPrediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>967</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1086</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1088</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1093</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1094</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1106</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1199</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1231</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1257</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Example  MyPrediction\n",
       "PassengerId                       \n",
       "893                1             0\n",
       "913                0             1\n",
       "925                1             0\n",
       "928                1             0\n",
       "956                0             1\n",
       "964                1             0\n",
       "967                0             1\n",
       "972                0             1\n",
       "980                1             0\n",
       "981                0             1\n",
       "1023               0             1\n",
       "1024               1             0\n",
       "1030               1             0\n",
       "1032               1             0\n",
       "1045               1             0\n",
       "1049               1             0\n",
       "1051               1             0\n",
       "1053               0             1\n",
       "1080               1             0\n",
       "1086               0             1\n",
       "1088               0             1\n",
       "1093               0             1\n",
       "1094               0             1\n",
       "1106               1             0\n",
       "1172               1             0\n",
       "1199               0             1\n",
       "1201               1             0\n",
       "1231               0             1\n",
       "1257               1             0\n",
       "1268               1             0\n",
       "1284               0             1\n",
       "1301               1             0\n",
       "1304               1             0\n",
       "1309               0             1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare = pd.read_csv(\"Dataset/gender_submission.csv\", index_col=0)\n",
    "compare = compare.rename(columns={\"Survived\": \"Example\"})\n",
    "compare = pd.concat([compare, results], axis=1)\n",
    "compare = compare.rename(columns={\"Survived\": \"MyPrediction\"})\n",
    "compare[compare.Example != compare.MyPrediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
